{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ],
      "metadata": {
        "id": "FCYPoiI_9iIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows you how to load-test the text recognizer endpoint you deployed via AWS Lambda. We'll use an open-source Python framework called Locust for this purpose. \n",
        "\n",
        "From the [documentation of of Locust](https://docs.locust.io/en/stable/what-is-locust.html):\n",
        "\n",
        "> Locust is an easy to use, scriptable and scalable performance testing tool.\n",
        "\n",
        "If you aren't familiar with Locust, be sure to check [this video](https://www.youtube.com/watch?t=163&v=Ok4x2LIbEEY&feature=emb_imp_woyt) out. "
      ],
      "metadata": {
        "id": "vid-FdfI9sSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "We just have a single Python dependency to install if you're running this on Colab. "
      ],
      "metadata": {
        "id": "9n1hImXp-Rhc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J36CobpsXEK"
      },
      "outputs": [],
      "source": [
        "!pip install -q locust"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Variables\n",
        "\n",
        "Here, we define paths for the\n",
        "\n",
        "* Locust script that will be used for conducting the load test. \n",
        "* Configuration file that will be used by the above script. "
      ],
      "metadata": {
        "id": "uFXLTiL1-Z5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_test_http_user = \"locust_http_user.py\"\n",
        "locust_conf = \"locust_fast_configs.conf\""
      ],
      "metadata": {
        "id": "7Z4Q3vyfwIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Locust Script for Load Testing"
      ],
      "metadata": {
        "id": "23Dg5sM4-7pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {load_test_http_user}\n",
        "\n",
        "from locust import HttpUser, constant, task\n",
        "import json\n",
        "import requests\n",
        "\n",
        "IMAGE_URI = \"https://fsdl-public-assets.s3-us-west-2.amazonaws.com/paragraphs/a01-077.png\"\n",
        "\n",
        "\n",
        "class TextRecognizerUser(HttpUser):\n",
        "    wait_time = constant(1)\n",
        "    headers = {\"Content-type\": \"application/json\"}\n",
        "    payload = json.dumps({\"image_url\": IMAGE_URI})\n",
        "\n",
        "    @task\n",
        "    def predict(self):\n",
        "        response = self.client.post(\"/\", data=self.payload, headers=self.headers)\n",
        "        pred = response.json()[\"pred\"]"
      ],
      "metadata": {
        "id": "GFufEahms0aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with the `TextRecognizerUser` class inherited from the `HttpUser` class. From the [documentation](https://docs.locust.io/en/stable/writing-a-locustfile.html#writing-a-locustfile):\n",
        "\n",
        "> [...] from HttpUser which gives each user a client attribute, which is an instance of HttpSession, that can be used to make HTTP requests to the target system that we want to load test. When a test starts, locust will create an instance of this class for every user that it simulates, and each of these users will start running within their own green gevent thread.\n",
        "\n",
        "Inside `TextRecognizerUser`, we define the task (decorated with `@task`) that we want to load test. In our case, this corresponds to calling our AWS Lambda endpoint and retrieving the predictions. We can define multiple tasks (each decorated with `@task`) like this and even weigh them differently. \n",
        "\n",
        "We have also defined a `wait_time` inside `TextRecognizerUser` that simulates a delay (in seconds) in between the requests. We can also provide sophisticated delay configurations here. For more information, consult [this resource](https://docs.locust.io/en/stable/writing-a-locustfile.html#wait-time). "
      ],
      "metadata": {
        "id": "w7l8Rkof_Fjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Locust Configuration"
      ],
      "metadata": {
        "id": "8M5AZ6NXBj1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {locust_conf}\n",
        "\n",
        "locustfile = \"locust_http_user.py\"\n",
        "headless = true\n",
        "users = 10\n",
        "spawn-rate = 1\n",
        "run-time = 5m\n",
        "host = https://3akxma777p53w57mmdika3sflu0fvazm.lambda-url.us-west-1.on.aws\n",
        "html = locust_report.html\n",
        "csv = locust_report"
      ],
      "metadata": {
        "id": "TLK6lDX_zcIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are brief descriptions of each of the variables we defined above:\n",
        "\n",
        "* `locustfile`: The Locust script that will be used for conducting the load test.\n",
        "* `headless`: When set to `true` Locust will run the load testing in background and will not use the web UI. When set to `false` Locust will start a web server on the post 8089. \n",
        "* `users`: Peak number of concurrent Locust users. We kept it to only 10 because we're using a public endpoint here. Heavier traffic might result in [DoS](https://en.wikipedia.org/wiki/Denial-of-service_attack).\n",
        "* `spawn-rate`: Number of users to spawn per second. \n",
        "* `run-time`: Total duration for running the load test. After the time has elapsed, Locust will stop. \n",
        "* `host`: Host where that houses the text recognizer endpoint. \n",
        "* `html`: Path to an HTML file that will contain aggregate information after the load test is complete.\n",
        "* `csv`: Prefix of CSV files to store current request stats.\n",
        "\n",
        "Locust provides many more options and you can read about them [here](https://docs.locust.io/en/stable/configuration.html).\n"
      ],
      "metadata": {
        "id": "Oa3S1G2uBrii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the Load Test"
      ],
      "metadata": {
        "id": "8eMXWYqhD9YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!locust --config={locust_conf}"
      ],
      "metadata": {
        "id": "lII173YEtDd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "After the load test is complete you should get the following files:"
      ],
      "metadata": {
        "id": "qWtNDSpmEE9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh locust_report*"
      ],
      "metadata": {
        "id": "I6GJhJtuEPGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can just download the `locust_report.html` file and take a look at the charts it provides. Below we see a table from the HTML file showing statistics on the request payloads and response times:\n",
        "\n",
        "![](https://i.ibb.co/997KF6R/aggregate-stats.png)\n",
        "\n",
        "The endpoint was load-tested with a total of 554 requests. It provides other useful information like:\n",
        "\n",
        "* Requests per second (`RPS`)\n",
        "* Average latency (`Average (ms)`)\n",
        "\n",
        "In a majority of real production scenarios, you'd want to reduce the average latency as minimum as possible and increase the RPS as high as possible. In the HTML file, we also get charts like so:\n",
        "\n",
        "![](https://i.ibb.co/MZfy6VR/charts.png)\n",
        "\n",
        "These charts help us study if there's any request failures and how the response time of the endpoint has changed over time. In our case, there were no request failures. However, request failures can happen when an endpoint is met with havy traffic. "
      ],
      "metadata": {
        "id": "Ob-XemXXES35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to derive the above stats programmatically, you can do so using the generated CSV files. For the purpose of this notebook, we'll only use `locust_report_stats_history.csv`. Some code in the below sections have been taken from [this notebook](https://github.com/sayakpaul/deploy-hf-tf-vision-models/blob/main/locust/load_test_results_vit_gke.ipynb). \n",
        "\n",
        "We start by importing `pandas` and `matplotlib` and then loading the CSV file into a Pandas Dataframe. "
      ],
      "metadata": {
        "id": "HXvf4IVEGFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TvAHI0Vq5oqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"locust_report_stats_history.csv\"\n",
        "report = pd.read_csv(csv_path)\n",
        "report.tail()"
      ],
      "metadata": {
        "id": "P6tZO-946N0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a few utility methods to tidy up the Dataframe a bit. "
      ],
      "metadata": {
        "id": "u_2k2NrgHzBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_be_updated_columns = [\n",
        "    {\n",
        "        \"old\": \"Total Average Response Time\",\n",
        "        \"new\": \"Total Average Response Time (ms)\"\n",
        "    },\n",
        "    {\n",
        "      \"old\": \"Total Max Response Time\",\n",
        "      \"new\": \"Total Max Response Time (ms)\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "MBJDo1Km5qG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_column_names(report, to_be_updated_columns):\n",
        "    for column in to_be_updated_columns:\n",
        "        report[column['new']] = report[column['old']]\n",
        "\n",
        "    return report\n",
        "\n",
        "def set_timestamp_offsets(report):\n",
        "    report[\"Timestamp\"] = report[\"Timestamp\"] - report.head(1)[\"Timestamp\"].tolist()[0]\n",
        "    return report"
      ],
      "metadata": {
        "id": "oPuQfx2B57j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = update_column_names(report, to_be_updated_columns)\n",
        "report = set_timestamp_offsets(report)"
      ],
      "metadata": {
        "id": "wLrfoztV6ddc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can plot the desired metrics in a comprehensive manner. "
      ],
      "metadata": {
        "id": "uJ0WZStmH4ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_graph(report, axis, title, x_axis=\"Timestamp\"):\n",
        "    y_axis = title\n",
        "    axis.set_title(title)\n",
        "\n",
        "    axis.plot(report[x_axis], report[y_axis])\n",
        "    axis.set_xlabel(x_axis)\n",
        "    return axis"
      ],
      "metadata": {
        "id": "4FR7wXNm6h2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure, axis = plt.subplots(3, 2, figsize=(30,30), dpi=80)\n",
        "\n",
        "axis[0, 0] = draw_graph(report, axis[0, 0], 'Total Request Count')\n",
        "axis[0, 1] = draw_graph(report, axis[0, 1], 'User Count')\n",
        "axis[1, 0] = draw_graph(report, axis[1, 0], 'Requests/s')\n",
        "axis[1, 1] = draw_graph(report, axis[1, 1], 'Total Failure Count')\n",
        "axis[2, 0] = draw_graph(report, axis[2, 0], 'Total Average Response Time (ms)')\n",
        "axis[2, 1] = draw_graph(report, axis[2, 1], 'Total Max Response Time (ms)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ap8257-i7TxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, we can see that the average response time of the endpoint has reduced till a certain point and after that it has plateaued. \n",
        "\n",
        "This method of visualization is often helpful when you have multiple CSV files (from multiple load tests of different configurations) to compare. "
      ],
      "metadata": {
        "id": "eHSkjawuH-oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concluding Notes\n",
        "\n",
        "In this notebook, we learned how to conduct a simple load test for our text recognizer endpoint. When developing endpoints for production use, you'd want to generate a more realistic traffic load. This is where distributed load testing will come handy. You can consult [this resource](https://docs.locust.io/en/stable/running-cloud-integration.html) in case you want to know more."
      ],
      "metadata": {
        "id": "KsI487qxIMKj"
      }
    }
  ]
}