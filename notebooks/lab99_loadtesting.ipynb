{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ],
      "metadata": {
        "id": "FCYPoiI_9iIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load-Testing with Locust"
      ],
      "metadata": {
        "id": "rsNyr52UtpQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows you how to load-test a Text Recognizer endpoint deployed via AWS Lambda.\n",
        "\n",
        "Load-testing is a form of testing that checks\n",
        "whether a web service can handle a particular\n",
        "_load_ of web traffic --\n",
        "quantity, timing, and content.\n",
        "\n",
        "We'll use an open-source Python framework called [Locust](https://docs.locust.io/en/stable/what-is-locust.html) for this purpose. \n",
        "\n",
        "From [their documentation](https://docs.locust.io/):\n",
        "\n",
        "> Locust is an easy to use, scriptable and scalable performance testing tool.\n",
        "\n",
        "For more on using Locust, check out [this video](https://www.youtube.com/watch?t=163&v=Ok4x2LIbEEY&feature=emb_imp_woyt). "
      ],
      "metadata": {
        "id": "vid-FdfI9sSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Locust is available as a Python package,\n",
        "so it's easy to install into our development environment."
      ],
      "metadata": {
        "id": "9n1hImXp-Rhc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J36CobpsXEK"
      },
      "outputs": [],
      "source": [
        "!pip install -q locust"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing a load test with `locust`"
      ],
      "metadata": {
        "id": "23Dg5sM4-7pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use Locust to simulate our users --\n",
        "creating a swarm of simple agents,\n",
        "like locusts.\n",
        "\n",
        "We define each user type as a class\n",
        "inside a `.py` file:"
      ],
      "metadata": {
        "id": "KqMcsN3wwvRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_test_file = \"locust_http_user.py\""
      ],
      "metadata": {
        "id": "RbQG71t0uVOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {load_test_file}\n",
        "\n",
        "from locust import HttpUser, constant, task\n",
        "import json\n",
        "import requests\n",
        "\n",
        "IMAGE_URI = \"https://fsdl-public-assets.s3-us-west-2.amazonaws.com/paragraphs/a01-077.png\"\n",
        "\n",
        "\n",
        "class TextRecognizerUser(HttpUser):\n",
        "    wait_time = constant(1)\n",
        "    headers = {\"Content-type\": \"application/json\"}\n",
        "    payload = json.dumps({\"image_url\": IMAGE_URI})\n",
        "\n",
        "    @task\n",
        "    def predict(self):\n",
        "        response = self.client.post(\"/\", data=self.payload, headers=self.headers)\n",
        "        pred = response.json()[\"pred\"]"
      ],
      "metadata": {
        "id": "GFufEahms0aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have only one type of user,\n",
        "the `TextRecognizerUser`,\n",
        "which inherits from the `HttpUser` class.\n",
        "\n",
        "From the [documentation](https://docs.locust.io/en/stable/writing-a-locustfile.html#writing-a-locustfile):\n",
        "\n",
        "> `HttpUser` ... gives each user a `client` attribute, which is an instance of `HttpSession`, that can be used to make HTTP requests to the target system that we want to load test. When a test starts, `locust` will create an instance of this class for every user that it simulates ...\n",
        "\n",
        "Inside `TextRecognizerUser`, we define the task (decorated with `@task`) that we want to load test. In our case, this corresponds to calling our AWS Lambda endpoint and retrieving the predictions.\n",
        "\n",
        "We have also defined a `wait_time` inside `TextRecognizerUser` that simulates a delay (in seconds) in between the requests. We can also provide sophisticated delay configurations here.\n",
        "\n",
        "For more information, consult [this resource](https://docs.locust.io/en/stable/writing-a-locustfile.html#wait-time). "
      ],
      "metadata": {
        "id": "w7l8Rkof_Fjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the load test"
      ],
      "metadata": {
        "id": "8M5AZ6NXBj1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run our load test with the `locust` command line tool.\n",
        "\n",
        "Run this cell, which takes about two minutes to complete,\n",
        "and continue reading below."
      ],
      "metadata": {
        "id": "9uCMMMZ2x6GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!locust --locustfile=locust_http_user.py \\\n",
        "  --headless \\\n",
        "  --users=10 \\\n",
        "  --spawn-rate=1 \\\n",
        "  --run-time=2m \\\n",
        "  --host=https://3akxma777p53w57mmdika3sflu0fvazm.lambda-url.us-west-1.on.aws \\\n",
        "  --html=locust_report.html \\\n",
        "  --csv=locust_report"
      ],
      "metadata": {
        "id": "lII173YEtDd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are brief descriptions of each of the arguments we use above:\n",
        "\n",
        "* `locustfile`: The Python file that will be used for conducting the load test.\n",
        "* `headless`: When set to `true` Locust will run the load testing in background and will not use the web UI. When set to `false` Locust will start a web server on the post 8089. \n",
        "* `users`: Peak number of concurrent simulated users.\n",
        "* `spawn-rate`: Number of users to spawn per second. \n",
        "* `run-time`: Total duration for running the load test. After the time has elapsed, Locust will stop. \n",
        "* `host`: The target for load testing. \n",
        "* `html`: Path to an HTML file that will contain aggregate information after the load test is complete.\n",
        "* `csv`: Prefix of CSV files to store current request stats.\n",
        "\n",
        "Locust provides many more options.\n",
        "\n",
        "It's generally better to store this information ina configuration file,\n",
        "so that results are more easily reproducible.\n",
        "\n",
        "You can read about configuring Locust [here](https://docs.locust.io/en/stable/configuration.html).\n"
      ],
      "metadata": {
        "id": "Oa3S1G2uBrii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viewing the results\n",
        "\n",
        "After the load test is complete,\n",
        "a number of report files will be generated:"
      ],
      "metadata": {
        "id": "qWtNDSpmEE9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh locust_report*"
      ],
      "metadata": {
        "id": "I6GJhJtuEPGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the HTML report directly in the notebook\n",
        "(or open it in the browser)."
      ],
      "metadata": {
        "id": "euj3GCik1Z7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display\n",
        "\n",
        "IPython.display.HTML(\"locust_report.html\")"
      ],
      "metadata": {
        "id": "PsJ5VSR2031m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we see some summary statistics on the user requests and our endpoint's responses.\n",
        "\n",
        "Here's what we saw:\n",
        "\n",
        "![](https://i.ibb.co/997KF6R/aggregate-stats.png)"
      ],
      "metadata": {
        "id": "SdXFw8na1wJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case above, the endpoint was load-tested with a total of 554 requests.\n",
        "\n",
        "This view provides other useful aggregate information like:\n",
        "\n",
        "* Requests per second (`RPS`)\n",
        "* Average latency (`Average (ms)`)\n",
        "\n",
        "In a majority of real production scenarios, you want to minimize the latency\n",
        "and maximize the RPS.\n",
        "\n",
        "We also want to watch the maximum response time,\n",
        "which can be a leading indicator of issues of our system\n",
        "or reveal issues that only a small minority of users encounter.\n",
        "\n",
        "Scrolling down, we find charts that show request and response data over time, like so:\n",
        "\n",
        "![](https://i.ibb.co/MZfy6VR/charts.png)"
      ],
      "metadata": {
        "id": "Ob-XemXXES35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These charts help us dig into failures and performance issues by showing them in greater detail.\n",
        "\n",
        "In our case, there were no request failures. However, request failures can happen when an endpoint is met with heavy traffic.\n",
        "\n",
        "We can also see the median and 95th percentile response times changing as requests come in.\n",
        "\n",
        "This is due to the auto-scaling behavior of AWS Lambdas\n",
        "(and other serverless cloud function systems).\n",
        "\n",
        "When the first few requests come in,\n",
        "the response times are very high.\n",
        "\n",
        "That's because in the absence of traffic,\n",
        "our system scales down to 0 --\n",
        "no machines are running.\n",
        "\n",
        "Getting started again when traffic arrives\n",
        "requires a bit of setup work for each machine running our Lambda.\n",
        "\n",
        "This setup work is visible in our data as a high median response time\n",
        "at the start that falls quickly\n",
        "(once half or more of the machines have been spun up)\n",
        "and an equally high 95th percentile response time\n",
        "that falls more slowly\n",
        "(once at least 95% of the machines have been spun up)."
      ],
      "metadata": {
        "id": "k6otTBBT2Vzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing load test data programmatically"
      ],
      "metadata": {
        "id": "tWKPiV1D2nSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Charts are nice for quick discovery,\n",
        "but they're hard to incorporate into automated workflows.\n",
        "\n",
        "We can also programmatically handle and analyze the data `locust` collected for us.\n",
        "\n",
        "We just need to read in the generated CSV files as `panbdas.DataFrames`.\n",
        "\n",
        "For the purpose of this notebook, we'll only use `locust_report_stats_history.csv`,\n",
        "and we'll just generate some charts to show that the data is present.\n",
        "\n",
        "Some code in the below sections have been adapted from\n",
        "[this notebook](https://github.com/sayakpaul/deploy-hf-tf-vision-models/blob/main/locust/load_test_results_vit_gke.ipynb),\n",
        "on testing a Vision Transformer deployed on Google Kubernetes Engine."
      ],
      "metadata": {
        "id": "HXvf4IVEGFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "\n",
        "csv_path = \"locust_report_stats_history.csv\"\n",
        "results = pd.read_csv(csv_path)\n",
        "results[\"Timestamp\"] = pd.to_datetime(results[\"Timestamp\"], unit=\"s\")\n",
        "results.tail()"
      ],
      "metadata": {
        "id": "TvAHI0Vq5oqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have total control over the plotted metrics:"
      ],
      "metadata": {
        "id": "uJ0WZStmH4ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request_columns = [\"Total Request Count\", \"Total Failure Count\", \"User Count\"]\n",
        "results.plot(x=\"Timestamp\", y=request_columns, subplots=True, sharey=True);"
      ],
      "metadata": {
        "id": "T4fsnyyQ4nvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_columns = [\"Total Average Response Time\", \"Total Max Response Time\"]\n",
        "results.plot(x=\"Timestamp\", y=response_columns);"
      ],
      "metadata": {
        "id": "rRcBVE4n5CQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we have total control over the calculation of statistics:"
      ],
      "metadata": {
        "id": "hLjlKcRl7_m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.groupby(\"Total Median Response Time\").describe()"
      ],
      "metadata": {
        "id": "fzgwTTz27qoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with the raw data yourself\n",
        "is often helpful when you have multiple CSV files\n",
        "(from multiple load tests of different configurations)\n",
        "to compare. "
      ],
      "metadata": {
        "id": "eHSkjawuH-oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and analyzing large-scale loads\n",
        "\n",
        "In this notebook,\n",
        "we learned how to conduct a simple load test for our text recognizer endpoint.\n",
        "\n",
        "When developing endpoints for large-scale production use,\n",
        "you'd want to generate traffic that is more realistic\n",
        "(e.g. comes from multiple hosts, not just one)\n",
        "and higher volume.\n",
        "\n",
        "This is where distributed load testing will come handy.\n",
        "\n",
        "You can consult [this resource](https://docs.locust.io/en/stable/running-cloud-integration.html) in case you want to know more."
      ],
      "metadata": {
        "id": "KsI487qxIMKj"
      }
    }
  ]
}