{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlH0lCOttCs5"
      },
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUPRHaeetRnT"
      },
      "source": [
        "# Lab 02a: PyTorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bry3Hr-PcgDs"
      },
      "source": [
        "### What You Will Learn\n",
        "\n",
        "- The core components of a PyTorch Lightning training loop: `LightningModule`s and `Trainer`s.\n",
        "- Useful quality-of-life improvements offered by PyTorch Lightning: `LightningDataModule`s, `Callback`s, and `Metric`s\n",
        "- How we use these features in the FSDL codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs0LXXlCU6Ix"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkQiK7lkgeXm"
      },
      "source": [
        "If you're running this notebook on Google Colab,\n",
        "the cell below will run full environment setup.\n",
        "\n",
        "It should take about three minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVx7C7H0PIZC"
      },
      "outputs": [],
      "source": [
        "lab_idx = 2\n",
        "\n",
        "if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "    # path management for Python\n",
        "    pythonpath, = !echo $PYTHONPATH\n",
        "    if \".\" not in pythonpath.split(\":\"):\n",
        "        pythonpath = \".:\" + pythonpath\n",
        "        %env PYTHONPATH={pythonpath}\n",
        "        !echo $PYTHONPATH\n",
        "\n",
        "    # get both Colab and local notebooks into the same state\n",
        "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
        "    import bootstrap\n",
        "\n",
        "    # change into the lab directory\n",
        "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
        "\n",
        "    # allow \"hot-reloading\" of modules\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
	"    # needed for inline plots in some contexts\n",
	"    %matplotlib inline\n",
        "\n",
        "    bootstrap.run = False  # change to True re-run setup\n",
        "    \n",
        "!pwd\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZN4bGgsgWc_"
      },
      "source": [
        "# Why Lightning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP8iJW_bg7IC"
      },
      "source": [
        "PyTorch is a powerful library for executing differentiable\n",
        "tensor operations with hardware acceleration\n",
        "and it includes many neural network primitives,\n",
        "but it has no concept of \"training\".\n",
        "At a high level, an `nn.Module` is a stateful function with gradients\n",
        "and a `torch.optim.Optimizer` can update that state using gradients,\n",
        "but there's no pre-built tools in PyTorch to iteratively generate those gradients from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7gIA-Efy91E"
      },
      "source": [
        "So the first thing many folks do in PyTorch is write that code --\n",
        "a \"training loop\" to iterate over their `DataLoader`,\n",
        "which in pseudocode might look something like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ewkWrwzDA8"
      },
      "source": [
        "```python\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = some_loss_function(targets, outputs)\n",
        "    \n",
        "    optimizer.zero_gradients()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYUtiJWize82"
      },
      "source": [
        "This is a solid start, but other needs immediately arise.\n",
        "You'll want to run your model on validation and test data,\n",
        "which need their own `DataLoader`s.\n",
        "Once finished, you'll want to save your model --\n",
        "and for long-running jobs, you probably want\n",
        "to save checkpoints of the training process\n",
        "so that it can be resumed in case of a crash.\n",
        "For state-of-the-art model performance in many domains,\n",
        "you'll want to distribute your training across multiple nodes/machines\n",
        "and across multiple GPUs within those nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0untumvjy5fm"
      },
      "source": [
        "That's just the tip of the iceberg, and you want\n",
        "all those features to work for lots of models and datasets,\n",
        "not just the one you're writing now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNPpi4OZjMbu"
      },
      "source": [
        "You don't want to write all of this yourself.\n",
        "\n",
        "So unless you are at a large organization that has a dedicated team\n",
        "for building that \"framework\" code,\n",
        "you'll want to use an existing library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnQuyVqUjJy8"
      },
      "source": [
        "PyTorch Lightning is a popular framework on top of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ecipNFTgZDt"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "version = pl.__version__\n",
        "\n",
        "docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/\"  # version can also be latest, stable\n",
        "docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE82xoEikWkh"
      },
      "source": [
        "At its core, PyTorch Lightning provides\n",
        "\n",
        "1. the `pl.Trainer` class, which organizes and executes your training, validation, and test loops, and\n",
        "2. the `pl.LightningModule` class, which links optimizers to models and defines how the model behaves during training, validation, and testing.\n",
        "\n",
        "Both of these are kitted out with all the features\n",
        "a cutting-edge deep learning codebase needs:\n",
        "- flags for switching device types and distributed computing strategy\n",
        "- saving, checkpointing, and resumption\n",
        "- calculation and logging of metrics\n",
        "\n",
        "and much more.\n",
        "\n",
        "Importantly these features can be easily\n",
        "added, removed, extended, or bypassed\n",
        "as desired, meaning your code isn't constrained by the framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuJUDmCeT3RK"
      },
      "source": [
        "In some ways, you can think of Lightning as a tool for \"organizing\" your PyTorch code,\n",
        "as shown in the video below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTt0TBs5TZpm"
      },
      "outputs": [],
      "source": [
        "import IPython.display as display\n",
        "\n",
        "\n",
        "display.IFrame(src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\",\n",
        "               width=720, height=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwpDn5GWn_X"
      },
      "source": [
        "That's opposed to the other way frameworks are designed,\n",
        "to provide abstractions over the lower-level library\n",
        "(here, PyTorch).\n",
        "\n",
        "Because of this \"organize don't abstract\" style,\n",
        "writing PyTorch Lightning code involves\n",
        "a lot of over-riding of methods --\n",
        "you inherit from a class\n",
        "and then implement the specific version of a general method\n",
        "that you need for your code,\n",
        "rather than Lightning providing a bunch of already\n",
        "fully-defined classes that you just instantiate,\n",
        "using arguments for configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXiUcQwan39S"
      },
      "source": [
        "# The `pl.LightningModule`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3FffD5Vn6we"
      },
      "source": [
        "The first of our two core classes,\n",
        "the `LightningModule`,\n",
        "is like a souped-up `torch.nn.Module` --\n",
        "it inherits all of the `Module` features,\n",
        "but adds more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QWwSStJTP28"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "issubclass(pl.LightningModule, torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1wiBVSTuHNT"
      },
      "source": [
        "To demonstrate how this class works,\n",
        "we'll build up a `LinearRegression` model dynamically,\n",
        "method by method.\n",
        "\n",
        "For this example we hard code lots of the details,\n",
        "but the real benefit comes when the details are configurable.\n",
        "\n",
        "In order to have a realistic example as well,\n",
        "we'll compare to the actual code\n",
        "in the `BaseLitModel` we use in the codebase\n",
        "as we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPARncfQ3ohz"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.lit_models import BaseLitModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myyL0vYU3z0a"
      },
      "source": [
        "A `pl.LightningModule` is a `torch.nn.Module`,\n",
        "so the basic definition looks the same:\n",
        "we need `__init__` and `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c0ylFO9rW_t"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
        "\n",
        "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
        "        self.model = torch.nn.Linear(in_features=1, out_features=1)\n",
        "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
        "\n",
        "    # optionally, define a forward method\n",
        "    def forward(self, xs):\n",
        "        return self.model(xs)  # we like to just call the model's forward method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY1yoGTy6CBu"
      },
      "source": [
        "But just the minimal definition for a `torch.nn.Module` isn't sufficient.\n",
        "\n",
        "If we try to use the class above with the `Trainer`, we get an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBWh_uHu5rmU"
      },
      "outputs": [],
      "source": [
        "import logging  # import some stdlib components to control what's display\n",
        "import textwrap\n",
        "import traceback\n",
        "\n",
        "\n",
        "try:  # try using the LinearRegression LightningModule defined above\n",
        "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)  # hide some info for now\n",
        "\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # we'll explain how the Trainer works in a bit\n",
        "    trainer = pl.Trainer(gpus=int(torch.cuda.is_available()), max_epochs=1)\n",
        "    trainer.fit(model=model)  \n",
        "\n",
        "except pl.utilities.exceptions.MisconfigurationException as error:\n",
        "    print(\"Error:\", *textwrap.wrap(str(error), 80), sep=\"\\n\\t\")  # show the error without raising it\n",
        "\n",
        "finally:  # bring back info-level logging\n",
        "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ni7xe5CgUt"
      },
      "source": [
        "The error message says we need some more methods.\n",
        "\n",
        "Two of them are mandatory components of the `LightningModule`: `.training_step` and `.configure_optimizers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37BXP7nAoBik"
      },
      "source": [
        "#### `.training_step`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah9MjWz2plFv"
      },
      "source": [
        "The `training_step` method defines,\n",
        "naturally enough,\n",
        "what to do during a single step of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plWEvWG_zRia"
      },
      "source": [
        "Roughly, it gets used like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RbxZ4idy-C5"
      },
      "source": [
        "```python\n",
        "\n",
        "# pseudocode modified from the Lightning documentation\n",
        "\n",
        "# put model in train mode\n",
        "model.train()\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    # run the train step\n",
        "    loss = training_step(batch)\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cemh_hGJ53nL"
      },
      "source": [
        "Effectively, it maps a batch to a loss value,\n",
        "so that PyTorch can backprop through that loss.\n",
        "\n",
        "The `.training_step` for our `LinearRegression` model is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8qW2VRRsPI2"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def training_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    xs, ys = batch  # unpack the batch\n",
        "    outs = self(xs)  # apply the model\n",
        "    loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "LinearRegression.training_step = training_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2e8m3BRCIx6"
      },
      "source": [
        "If you've written PyTorch code before, you'll notice that we don't mention devices\n",
        "or other tensor metadata here -- that's handled for us by Lightning, which is a huge relief."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkvNpfwqpns5"
      },
      "source": [
        "You can additionally define\n",
        "a `validation_step` and a `test_step`\n",
        "to define the model's behavior during\n",
        "validation and testing loops.\n",
        "\n",
        "You're invited to define these steps\n",
        "in the exercises at the end of the lab.\n",
        "\n",
        "Inside this step is also where you might calculate other\n",
        "values related to inputs, outputs, and loss,\n",
        "like non-differentiable metrics (e.g. accuracy, precision, recall).\n",
        "\n",
        "So our `BaseLitModel`'s got a slightly more complex `training_step` method,\n",
        "and the details of the forward pass are deferred to `._run_on_batch` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpBkRczao1hr"
      },
      "outputs": [],
      "source": [
        "BaseLitModel.training_step??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhoYf_NoEyc"
      },
      "source": [
        "#### `.configure_optimizers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCIAWoCEtIU7"
      },
      "source": [
        "Thanks to `training_step` we've got a loss, and PyTorch can turn that into a gradient.\n",
        "\n",
        "But we need more than a gradient to do an update.\n",
        "\n",
        "We need an _optimizer_ that can make use of the gradients to update the parameters. In complex cases, we might need more than one optimizer (e.g. GANs).\n",
        "\n",
        "Our second required method, `.configure_optimizers`,\n",
        "sets up the `torch.optim.Optimizer`s \n",
        "(e.g. setting their hyperparameters\n",
        "and pointing them at the `Module`'s parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMlnRdIPzvDF"
      },
      "source": [
        "In psuedo-code (modified from the Lightning documentation), it gets used something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBnfJzszi49"
      },
      "source": [
        "```python\n",
        "optimizer = model.configure_optimizers()\n",
        "\n",
        "for batch_idx, batch in enumerate(data):\n",
        "\n",
        "    def closure():  # wrap the loss calculation\n",
        "        loss = model.training_step(batch, batch_idx, ...)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    # optimizer can call the loss calculation as many times as it likes\n",
        "    optimizer.step(closure)  # some optimizers need this, like (L)-BFGS\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsP3DBy7YzW"
      },
      "source": [
        "For our `LinearRegression` model,\n",
        "we just need to instantiate an optimizer and point it at the parameters of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWrWGgdVt21h"
      },
      "outputs": [],
      "source": [
        "def configure_optimizers(self: LinearRegression) -> torch.optim.Optimizer:\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)  # https://fsdl.me/ol-reliable-img\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "LinearRegression.configure_optimizers = configure_optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta2hs0OLwbtF"
      },
      "source": [
        "You can read more about optimization in Lightning,\n",
        "including how to manually control optimization\n",
        "instead of relying on default behavior,\n",
        "in the docs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXINqlAgwfKy"
      },
      "outputs": [],
      "source": [
        "optimization_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/optimization.html\"\n",
        "optimization_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWdKdZDfxmb2"
      },
      "source": [
        "The `configure_optimizers` method for the `BaseLitModel`\n",
        "isn't that much more complex.\n",
        "\n",
        "We just add support for learning rate schedulers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyRbz0bEpWwd"
      },
      "outputs": [],
      "source": [
        "BaseLitModel.configure_optimizers??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQCfn7Nm_QP"
      },
      "source": [
        "# The `pl.Trainer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RScc0ef97qlc"
      },
      "source": [
        "The `LightningModule` has already helped us organize our code,\n",
        "but it's not really useful until we combine it with the `Trainer`,\n",
        "which relies on the `LightningModule` interface to execute training, validation, and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBdikPBF86Qp"
      },
      "source": [
        "The `Trainer` is where we make choices like how long to train\n",
        "(`max_epochs`, `min_epochs`, `max_time`, `max_steps`),\n",
        "what kind of acceleration (e.g. `gpus`) or distribution strategy to use,\n",
        "and other settings that might differ across training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ4KSdFP3E4Q"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(max_epochs=20, gpus=int(torch.cuda.is_available()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2l3rGZK7-PL"
      },
      "source": [
        "Before we can actually use the `Trainer`, though,\n",
        "we also need a `torch.utils.data.DataLoader` --\n",
        "nothing new from PyTorch Lightning here,\n",
        "just vanilla PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcUSD2jP4Ffo"
      },
      "outputs": [],
      "source": [
        "class CorrelatedDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, N=10_000):\n",
        "        self.N = N\n",
        "        self.xs = torch.randn(size=(N, 1))\n",
        "        self.ys = torch.randn_like(self.xs) + self.xs  # correlated target data: y ~ N(x, 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.xs[idx], self.ys[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "dataset = CorrelatedDataset()\n",
        "tdl = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0u41JtA8qGo"
      },
      "source": [
        "We can fetch some sample data from the `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1j6Gj9Ka0dJ"
      },
      "outputs": [],
      "source": [
        "example_xs, example_ys = next(iter(tdl))  # grabbing an example batch to print\n",
        "\n",
        "print(\"xs:\", example_xs[:10], sep=\"\\n\")\n",
        "print(\"ys:\", example_ys[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnqk3mRv8dbW"
      },
      "source": [
        "and, since it's low-dimensional, visualize it\n",
        "and see what we're asking the model to learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33jcHbErbl6Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
        "  .plot(x=\"x\", y=\"y\", kind=\"scatter\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA7-4tJJ9fde"
      },
      "source": [
        "Now we're ready to run training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY910O803oPU"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
        "\n",
        "trainer.fit(model=model, train_dataloaders=tdl)\n",
        "\n",
        "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQBXYmLF_GoI"
      },
      "source": [
        "The loss after training should be less than the loss before training,\n",
        "and we can see that our model's predictions line up with the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqcbA91x96-s"
      },
      "outputs": [],
      "source": [
        "ax = pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
        "  .plot(x=\"x\", y=\"y\", legend=True, kind=\"scatter\", label=\"data\")\n",
        "\n",
        "inps = torch.arange(-2, 2, 0.5)[:, None]\n",
        "ax.plot(inps, model(inps).detach(), lw=2, color=\"k\", label=\"predictions\"); ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZkpsNfl3P8R"
      },
      "source": [
        "The `Trainer` promises to \"customize every aspect of training via flags\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q-c9b62_XFj"
      },
      "outputs": [],
      "source": [
        "pl.Trainer.__init__.__doc__.strip().split(\"\\n\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He-zEwMB_oKH"
      },
      "source": [
        "and they mean _every_ aspect.\n",
        "\n",
        "The cell below prints all of the arguments for the `pl.Trainer` class --\n",
        "no need to memorize or even understand them all now,\n",
        "just skim it to see how many customization options there are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F_rRPL3lfPE"
      },
      "outputs": [],
      "source": [
        "print(pl.Trainer.__init__.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X8dGmR53kYU"
      },
      "source": [
        "It's probably easier to read them on the documentation website:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqUj6MxRkppr"
      },
      "outputs": [],
      "source": [
        "trainer_docs_link = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/trainer.html\"\n",
        "trainer_docs_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T8XMYvr__Y5"
      },
      "source": [
        "# Training with PyTorch Lightning in the FSDL Codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CtaPliTAxy3"
      },
      "source": [
        "The `LightningModule`s in the FSDL codebase\n",
        "are stored in the `lit_models` submodule of the `text_recognizer` module.\n",
        "\n",
        "For now, we've just got some basic models.\n",
        "We'll add more as we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMe5z1RSAyo_"
      },
      "outputs": [],
      "source": [
        "!ls text_recognizer/lit_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZTYmIHbBu7g"
      },
      "source": [
        "We also have a folder called `training` now.\n",
        "\n",
        "This contains a script, `run_experiment.py`,\n",
        "that is used for running training jobs.\n",
        "\n",
        "In case you want to play around with the training code\n",
        "in a notebook, you can also load it as a module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRz9GbXzNJLM"
      },
      "outputs": [],
      "source": [
        "!ls training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im9vLeyqBv_h"
      },
      "outputs": [],
      "source": [
        "import training.run_experiment\n",
        "\n",
        "\n",
        "print(training.run_experiment.__doc__, training.run_experiment.main.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hcAXqHAV0v"
      },
      "source": [
        "We build the `Trainer` from command line arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi50CDZul7Mm"
      },
      "outputs": [],
      "source": [
        "# how the trainer is initialized in the training script\n",
        "!grep \"pl.Trainer.from\" training/run_experiment.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQheYJyAxlh"
      },
      "source": [
        "so all the configuration flexibility and complexity of the `Trainer`\n",
        "is available via the command line.\n",
        "\n",
        "Docs for the command line arguments for the trainer are accessible with `--help`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlSmSyCMAw7Z"
      },
      "outputs": [],
      "source": [
        "# displays the first few flags for controlling the Trainer from the command line\n",
        "!python training/run_experiment.py --help | grep \"pl.Trainer\" -A 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIZ_VRPcNMsM"
      },
      "source": [
        "We'll use `run_experiment` in\n",
        "[Lab 02b](http://fsdl.me/lab02b-colab)\n",
        "to train convolutional neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0siaL4Qumc_"
      },
      "source": [
        "# Extra Goodies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQSPnxQDBF6"
      },
      "source": [
        "The `LightningModule` and the `Trainer` are the minimum amount you need\n",
        "to get started with PyTorch Lightning.\n",
        "\n",
        "But they aren't all you need.\n",
        "\n",
        "There are many more features built into Lightning and its ecosystem.\n",
        "\n",
        "We'll cover three more here:\n",
        "- `pl.LightningDataModule`s, for organizing dataloaders and handling data in distributed settings\n",
        "- `pl.Callback`s, for adding \"optional\" extra features to model training\n",
        "- `torchmetrics`, for efficiently computing and logging "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOYHSLw_D8Zy"
      },
      "source": [
        "## `pl.LightningDataModule`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjTNGzREIpl"
      },
      "source": [
        "Where the `LightningModule` organizes our model and its optimizers,\n",
        "the `LightningDataModule` organizes our dataloading code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_KkQ0iOWKD7"
      },
      "source": [
        "The class-level docstring explains the concept\n",
        "behind the class well\n",
        "and lists the main methods to be over-ridden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFTWHdsFV5WG"
      },
      "outputs": [],
      "source": [
        "print(pl.LightningDataModule.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiacppGB9BB"
      },
      "source": [
        "Let's upgrade our `CorrelatedDataset` from a PyTorch `Dataset` to a `LightningDataModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1d62iC6Xv1i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class CorrelatedDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, size=10_000, train_frac=0.8, batch_size=32):\n",
        "        super().__init__()  # again, mandatory superclass init, as with torch.nn.Modules\n",
        "\n",
        "        # set some constants, like the train/val split\n",
        "        self.size = size\n",
        "        self.train_frac, self.val_frac = train_frac, 1 - train_frac\n",
        "        self.train_indices = list(range(math.floor(self.size * train_frac)))\n",
        "        self.val_indices = list(range(self.train_indices[-1], self.size))\n",
        "\n",
        "        # under the hood, we've still got a torch Dataset\n",
        "        self.dataset = CorrelatedDataset(N=size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQf-jUYRCi3m"
      },
      "source": [
        "`LightningDataModule`s are designed to work in distributed settings,\n",
        "where operations that set state\n",
        "(e.g. writing to disk or attaching something to `self` that you want to access later)\n",
        "need to be handled with care.\n",
        "\n",
        "Getting data ready for training is often a very stateful operation,\n",
        "so the `LightningDataModule` provides two separate methods for it:\n",
        "one called `setup` that handles any state that needs to be set up in each copy of the module\n",
        "(here, splitting the data and adding it to `self`)\n",
        "and one called `prepare_data` that handles any state that only needs to be set up in each machine\n",
        "(for example, downloading data from storage and writing it to the local disk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mttu--rHX70r"
      },
      "outputs": [],
      "source": [
        "def setup(self, stage=None):  # prepares state that needs to be set for each GPU on each node\n",
        "    if stage == \"fit\" or stage is None:  # other stages: \"test\", \"predict\"\n",
        "        self.train_dataset = torch.utils.data.Subset(self.dataset, self.train_indices)\n",
        "        self.val_dataset = torch.utils.data.Subset(self.dataset, self.val_indices)\n",
        "\n",
        "def prepare_data(self):  # prepares state that needs to be set once per node\n",
        "    pass  # but we don't have any \"node-level\" computations\n",
        "\n",
        "\n",
        "CorrelatedDataModule.setup, CorrelatedDataModule.prepare_data = setup, prepare_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh3mZrjwD83Y"
      },
      "source": [
        "We then define methods to return `DataLoader`s when requested by the `Trainer`.\n",
        "\n",
        "To run a testing loop that uses a `LightningDataModule`,\n",
        "you'll also need to define a `test_dataloader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu9Ma3iKYPBd"
      },
      "outputs": [],
      "source": [
        "def train_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "    return torch.utils.data.DataLoader(self.train_dataset, batch_size=32)\n",
        "\n",
        "def val_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "    return torch.utils.data.DataLoader(self.val_dataset, batch_size=32)\n",
        "\n",
        "CorrelatedDataModule.train_dataloader, CorrelatedDataModule.val_dataloader = train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNodiN6oawX5"
      },
      "source": [
        "Now we're ready to run training using a datamodule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKBwoE-Rajqw"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "trainer.fit(model=model, datamodule=datamodule)\n",
        "\n",
        "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6flh5Jf2ZP"
      },
      "source": [
        "Notice the warning: \"`Skipping val loop.`\"\n",
        "\n",
        "It's being raised because our minimal `LinearRegression` model\n",
        "doesn't have a `.validation_step` method.\n",
        "\n",
        "In the exercises, you're invited to add a validation step and resolve this warning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJnoFx47ZjBw"
      },
      "source": [
        "In the FSDL codebase,\n",
        "we define the basic functions of a `LightningDataModule`\n",
        "in the `BaseDataModule` and defer details to subclasses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTPKvDDGXmOr"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.data import BaseDataModule\n",
        "\n",
        "\n",
        "BaseDataModule??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mRlZecwaKB4"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.data.mnist import MNIST\n",
        "\n",
        "\n",
        "MNIST??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQbMY08qD-hm"
      },
      "source": [
        "## `pl.Callback`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVe7TSNvHK4K"
      },
      "source": [
        "Lightning's `Callback` class is used to add \"nice-to-have\" features\n",
        "to training, validation, and testing\n",
        "that aren't strictly necessary for any model to run\n",
        "but are useful for many models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzU76wgFGw9N"
      },
      "source": [
        "A \"callback\" is a unit of code that's meant to be called later,\n",
        "based on some trigger.\n",
        "\n",
        "It's a very flexible system, which is why\n",
        "`Callback`s are used internally to implement lots of important Lightning features,\n",
        "including some we've already discussed, like `ModelCheckpoint` for saving during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-msDjbKdHTxU"
      },
      "outputs": [],
      "source": [
        "pl.callbacks.__all__  # builtin Callbacks from Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6WRNXtHHkbM"
      },
      "source": [
        "The triggers, or \"hooks\", here, are specific points in the training, validation, and testing loop.\n",
        "\n",
        "The names of the hooks generally explain when the hook will be called,\n",
        "but you can always check the documentation for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iHjjnU8Hvgg"
      },
      "outputs": [],
      "source": [
        "hooks = \", \".join([method for method in dir(pl.Callback) if method.startswith(\"on_\")])\n",
        "print(\"hooks:\", *textwrap.wrap(hooks, width=80), sep=\"\\n\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E2M7O2cGdj7"
      },
      "source": [
        "You can define your own `Callback` by inheriting from `pl.Callback`\n",
        "and over-riding one of the \"hook\" methods --\n",
        "much the same way that you define your own `LightningModule`\n",
        "by writing your own `.training_step` and `.configure_optimizers`.\n",
        "\n",
        "Let's define a silly `Callback` just to demonstrate the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UodFQKAGEJlk"
      },
      "outputs": [],
      "source": [
        "class HelloWorldCallback(pl.Callback):\n",
        "\n",
        "    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        print(\"👋 hello from the start of the training epoch!\")\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        print(\"👋 hello from the end of the validation epoch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7oIpyEGoaP"
      },
      "source": [
        "This callback will print a message whenever the training epoch starts\n",
        "and whenever the validation epoch ends.\n",
        "\n",
        "Different \"hooks\" have different information directly available.\n",
        "\n",
        "For example, you can directly access the batch information\n",
        "inside the `on_train_batch_start` and `on_train_batch_end` hooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U17Qo_i_GCya"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def on_train_batch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
        "        if random.random() > 0.995:\n",
        "            print(f\"👋 hello from inside the lucky batch, #{batch_idx}!\")\n",
        "\n",
        "\n",
        "HelloWorldCallback.on_train_batch_start = on_train_batch_start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVKQXZOwQNGJ"
      },
      "source": [
        "We provide the callbacks when initializing the `Trainer`,\n",
        "then they are invoked during model fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XHXZ64-ETCz"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "trainer = pl.Trainer(  # we instantiate and provide the callback here, but nothing happens yet\n",
        "    max_epochs=10, gpus=int(torch.cuda.is_available()), callbacks=[HelloWorldCallback()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEHUUhVOQv6K"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP2Xj1woFGwG"
      },
      "source": [
        "You can read more about callbacks in the documentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COHk5BZvFJN_"
      },
      "outputs": [],
      "source": [
        "callback_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/extensions/callbacks.html\"\n",
        "callback_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2K9e44iEGCR"
      },
      "source": [
        "## `torchmetrics`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO-UIFKyJCqJ"
      },
      "source": [
        "DNNs are also finicky and break silently:\n",
        "rather than crashing, they just start doing the wrong thing.\n",
        "Without careful monitoring, that wrong thing can be invisible\n",
        "until long after it has done a lot of damage to you, your team, or your users.\n",
        "\n",
        "We want to calculate metrics so we can monitor what's happening during training and catch bugs --\n",
        "or even achieve [\"observability\"](https://thenewstack.io/observability-a-3-year-retrospective/),\n",
        "meaning we can also determine\n",
        "how to fix bugs in training just by viewing logs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4YMyUI0Jr2f"
      },
      "source": [
        "But DNN training is also performance sensitive.\n",
        "Training runs for large language models have budgets that are\n",
        "more comparable to building an apartment complex\n",
        "than they are to the build jobs of traditional software pipelines.\n",
        "\n",
        "Slowing down training even a small amount can add a substantial dollar cost,\n",
        "obviating the benefits of catching and fixing bugs more quickly.\n",
        "\n",
        "Also implementing metric calculation during training adds extra work,\n",
        "much like the other software engineering best practices which it closely resembles,\n",
        "namely test-writing and monitoring.\n",
        "This distracts and detracts from higher-leverage research work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvWjiHSIxzM"
      },
      "source": [
        "\n",
        "The `torchmetrics` library, which began its life as `pytorch_lightning.metrics`,\n",
        "resolves these issues by providing a `Metric` class that\n",
        "incorporates best performance practices,\n",
        "like smart accumulation across batches and over devices,\n",
        "defines a unified interface,\n",
        "and integrates with Lightning's built-in logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21y3lgvwEKPC"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "\n",
        "tm_version = torchmetrics.__version__\n",
        "print(\"metrics:\", *textwrap.wrap(\", \".join(torchmetrics.__all__), width=80), sep=\"\\n\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TuPZkV1gfFE"
      },
      "source": [
        "Like the `LightningModule`, `torchmetrics.Metric` inherits from `torch.nn.Module`.\n",
        "\n",
        "That's because metric calculation, like module application, is typically\n",
        "1) an array-heavy computation that\n",
        "2) relies on persistent state\n",
        "(parameters for `Module`s, running values for `Metric`s) and\n",
        "3) benefits from acceleration and\n",
        "4) can be distributed over devices and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leiiI_QDS2_V"
      },
      "outputs": [],
      "source": [
        "issubclass(torchmetrics.Metric, torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy8MF2taP8MV"
      },
      "source": [
        "Documentation for the version of `torchmetrics` we're using can be found here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN4ashooP_tM"
      },
      "outputs": [],
      "source": [
        "torchmetrics_docs_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/\"\n",
        "torchmetrics_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aycHhZNXwjr"
      },
      "source": [
        "In the `BaseLitModel`,\n",
        "we use the `torchmetrics.Accuracy` metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyq4IjmBXzTv"
      },
      "outputs": [],
      "source": [
        "BaseLitModel.__init__??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPoTH50YfkMF"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD_6PVAeflWw"
      },
      "source": [
        "### 🌟 Add a `validation_step` to the `LinearRegression` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KKbAN9eK281"
      },
      "outputs": [],
      "source": [
        "def validation_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    pass # your code here\n",
        "\n",
        "\n",
        "LinearRegression.validation_step = validation_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnPPHAPxFCEv"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "# if you code is working, you should see results for the validation loss in the output\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u42zXktOFDhZ"
      },
      "source": [
        "### 🌟🌟 Add a `test_step` to the `LinearRegression` class and a `test_dataloader` to the `CorrelatedDataModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbWfqvumFESV"
      },
      "outputs": [],
      "source": [
        "def test_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    pass # your code here\n",
        "\n",
        "LinearRegression.test_step = test_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB96MpibLeJi"
      },
      "outputs": [],
      "source": [
        "class CorrelatedDataModuleWithTest(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, N=10_000, N_test=10_000):  # reimplement __init__ here\n",
        "        super().__init__()  # don't forget this!\n",
        "        self.dataset = None\n",
        "        self.test_dataset = None  # define a test set -- another sample from the same distribution\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        pass\n",
        "\n",
        "    def test_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "        pass  # create a dataloader for the test set here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jq3dcugMMOu"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModuleWithTest()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# we run testing without fitting here\n",
        "trainer.test(model=model, datamodule=datamodule)  # if your code is working, you should see performance on the test set here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHg4MKmJPla6"
      },
      "source": [
        "### 🌟🌟🌟 Make a version of the `LinearRegression` class that calculates the `ExplainedVariance` metric during training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_1AKGWRR2ai"
      },
      "source": [
        "The \"variance explained\" is a useful metric for comparing regression models --\n",
        "its values are interpretable and comparable across datasets, unlike raw loss values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLecK4CsQWKk"
      },
      "source": [
        "Read the \"TorchMetrics in PyTorch Lightning\" guide for details on how to\n",
        "add metrics and metric logging\n",
        "to a `LightningModule`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWy0HyG4RYnX"
      },
      "outputs": [],
      "source": [
        "torchmetrics_guide_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/pages/lightning.html\"\n",
        "torchmetrics_guide_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoSQ3y6sSTvP"
      },
      "source": [
        "And check out the docs for `ExplainedVariance` to see how it's calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpGuRK2FRHh1"
      },
      "outputs": [],
      "source": [
        "print(torchmetrics.ExplainedVariance.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EAtpWXrSVR1"
      },
      "source": [
        "You'll want to start the `LinearRegression` class over from scratch,\n",
        "since the `__init__` and `{training, validation, test}_step` methods need to be rewritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGtWt3_5SYTn"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFWNr1SfS5-r"
      },
      "source": [
        "You can test your code by running fitting and testing.\n",
        "\n",
        "To see whether it's working,\n",
        "[call `self.log` inside the `_step` methods](https://torchmetrics.readthedocs.io/en/v0.7.1/pages/lightning.html)\n",
        "with the\n",
        "[keyword argument `prog_bar=True`](https://pytorch-lightning.readthedocs.io/en/1.6.1/api/pytorch_lightning.core.LightningModule.html#pytorch_lightning.core.LightningModule.log).\n",
        "You should see the explained variance show up in the output alongside the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jse95DGCS6gR",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# if your code is working, you should see explained variance in the progress bar/logs\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab02a_lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
