{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 03: Transformers and Paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- The fundamental reasons why the Transformer is such\n",
    "a powerful and popular architecture\n",
    "- Core intuitions for the behavior of Transformer architectures\n",
    "- How to use a convolutional encoder and a Transformer decoder to recognize\n",
    "entire paragraphs of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 3\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZN4bGgsgWc_"
   },
   "source": [
    "# Why Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in building a text recognizer is to take a two-dimensional image\n",
    "and convert it into a one-dimensional sequence of characters\n",
    "from some alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks,\n",
    "discussed in [Lab 02b](https://fsdl.me/lab02b-colab),\n",
    "are great at encoding images,\n",
    "taking them from their raw pixel values\n",
    "to a more semantically meaningful numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we go from that to a sequence of letters?\n",
    "And what's especially tricky:\n",
    "the number of letters in an image is separable from its size.\n",
    "A screenshot of this document has a much higher density of letters\n",
    "than a close-up photograph of a piece of paper.\n",
    "How do we get a _variable-length_ sequence of letters,\n",
    "where the length need have nothing to do with the size of the input tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Transformers_ are an encoder-decoder architecture that excels at sequence modeling --\n",
    "they were\n",
    "[originally introduced](https://arxiv.org/abs/1706.03762)\n",
    "for transforming one sequence into another,\n",
    "as in machine translation.\n",
    "This makes them a natural fit for processing language.\n",
    "\n",
    "But they have also found success in other domains --\n",
    "at the time of this writing, large transformers\n",
    "dominate the\n",
    "[ImageNet classification benchmark](https://paperswithcode.com/sota/image-classification-on-imagenet)\n",
    "that has become a de facto standard for comparing models\n",
    "and are finding\n",
    "[application in reinforcement learning](https://arxiv.org/abs/2106.01345)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will use a Transformer as a key component of our final architecture:\n",
    "we will encode our input images with a CNN\n",
    "and then read them out into a text sequence with a Transformer.\n",
    "\n",
    "Before trying out this new model,\n",
    "let's first get an understanding of why the Transformer architecture\n",
    "has become so popular by walking through its history\n",
    "and then get some intuition for how it works\n",
    "by looking at some\n",
    "[recent work](https://transformer-circuits.pub/)\n",
    "on explaining the behavior of both toy models and state-of-the-art language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmKqjbvd-Mj3"
   },
   "source": [
    "## Why not convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRqkUMdM-OxU"
   },
   "source": [
    "In the ancient beforetimes (i.e. 2016),\n",
    "the best models for natural language processing were all\n",
    "_recurrent_ neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks were also occasionally used,\n",
    "but they suffered from a serious issue:\n",
    "their architectural biases don't fit text.\n",
    "\n",
    "First, _translation equivariance_ no longer holds.\n",
    "The beginning of a piece of text is often quite different from the middle,\n",
    "so the absolute position matters.\n",
    "\n",
    "Second, _locality_ is not as important in language.\n",
    "The name of a character that hasn't appeared in thousands of pages\n",
    "can become salient when someone asks, \"Whatever happened to\n",
    "[Radagast the Brown](https://tvtropes.org/pmwiki/pmwiki.php/ChuckCunninghamSyndrome/Literature)?\"\n",
    "\n",
    "Consider interpreting a piece of text like the Python code below:\n",
    "```python\n",
    "def do(arg1, arg2, arg3):\n",
    "    a = arg1 + arg2\n",
    "    b = arg3[:3]\n",
    "    c = a * b\n",
    "    return c\n",
    "\n",
    "print(do(1, 1, \"ayy lmao\"))\n",
    "```\n",
    "\n",
    "After a `(` we expect a `)`,\n",
    "but possibly very long afterwards,\n",
    "[e.g. in the definition of `pl.Trainer.__init__`](https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/trainer/trainer.html#Trainer.__init__),\n",
    "and similarly we expect a `]` at some point after a `[`.\n",
    "\n",
    "For translation variance, consider\n",
    "that we interpret `*` not by\n",
    "comparing it to its neighbors\n",
    "but by looking at `a` and `b`.\n",
    "We mix knowledge learned through experience\n",
    "with new facts learned while reading --\n",
    "also known as _in-context learning_.\n",
    "\n",
    "In a longer text,\n",
    "[e.g. the one you are reading now](./lab03_transformers.ipynb),\n",
    "the translation variance of text is clearer.\n",
    "Every lab notebook begins with the same header,\n",
    "setting up the environment,\n",
    "but that header never appears elsewhere in the notebook.\n",
    "Later positions need to be processed in terms of the previous entries.\n",
    "\n",
    "Unlike an image, we cannot simply rotate or translate our \"camera\"\n",
    "and get a new valid text.\n",
    "[Rare is the book](https://en.wikipedia.org/wiki/Dictionary_of_the_Khazars)\n",
    "that can be read without regard to position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of formal language theory,\n",
    "which has deep mutual influence with computer science,\n",
    "gives one way of explaining the issues with convolutional networks:\n",
    "they can only understand languages with _finite contexts_,\n",
    "where all the information can be found within a finite window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate solution, drawing from the connections to computer science, is\n",
    "[recursion](https://www.google.com/search?q=recursion).\n",
    "A network whose output on the final entry of the sequence is a recursive function\n",
    "of all the previous entries can build up knowledge\n",
    "as it reads the sequence and treat early entries quite differently than it does late ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa6cbTlImkEh"
   },
   "source": [
    "In pseudo-code, such a _recurrent neural network_ module might look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKtBoPnglPrW"
   },
   "source": [
    "```python\n",
    "def recurrent_module(xs: torch.Tensor[\"S\", \"input_dims\"]) -> torch.Tensor[\"feature_dims\"]:\n",
    "    next_inputs = input_module(xs[-1])\n",
    "    next_hiddens = feature_module(recurrent_module(xs[:-1]))  # recursive call\n",
    "    return output_module(next_inputs, next_hiddens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbJPSMnEm516"
   },
   "source": [
    "If you've had formal computer science training,\n",
    "then you may be familiar with the power of recursion,\n",
    "e.g. the\n",
    "[Y-combinator](https://en.wikipedia.org/wiki/Fixed-point_combinator#Y_combinator)\n",
    "that gave its name to the now much better-known\n",
    "[startup incubator](https://www.ycombinator.com/).\n",
    "\n",
    "The particular form of recursion used by\n",
    "recurrent neural networks implements a\n",
    "[reduce-like operation](https://colah.github.io/posts/2015-09-NN-Types-FP/).\n",
    "\n",
    "> <small> If you've know a lot of computer science,\n",
    "you might be concerned by this connection.\n",
    "What about other\n",
    "[recursion schemes](https://blog.sumtypeofway.com/posts/introduction-to-recursion-schemes.html)?\n",
    "Where are the neural network architectures for differentiable\n",
    "[zygohistomorphic prepromorphisms](https://wiki.haskell.org/Zygohistomorphic_prepromorphisms)?\n",
    "Check out Graph Neural Networks,\n",
    "[which implement dynamic programming](https://arxiv.org/abs/2203.15544).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63mMTbEBpVuE"
   },
   "source": [
    "Recurrent networks are able to achieve\n",
    "[decent results in language modeling and machine translation](https://paperswithcode.com/paper/regularizing-and-optimizing-lstm-language).\n",
    "\n",
    "There are many popular recurrent architectures,\n",
    "from the beefy and classic\n",
    "[LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n",
    "and the svelte and modern [GRU](https://arxiv.org/abs/1412.3555)\n",
    "([no relation](https://fsdl-public-assets.s3.us-west-2.amazonaws.com/gru.jpeg)),\n",
    "all of which have roughly similar capabilities but\n",
    "[some of which are easier to train](https://arxiv.org/abs/1611.09913)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwQHVTIslOku"
   },
   "source": [
    "In the same sense that MLPs can model \"any\" feedforward function,\n",
    "in principle even basic RNNs\n",
    "[can model \"any\" dynamical system](https://www.sciencedirect.com/science/article/abs/pii/S089360800580125X).\n",
    "\n",
    "In particular they can model any\n",
    "[Turing machine](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis),\n",
    "which is a formal way of saying that they can in principle\n",
    "do anything a computer is capable of doing.\n",
    "\n",
    "The question is then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J8EoGN3pu7P"
   },
   "source": [
    "## Why aren't we all using RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDwNWaevpt_3"
   },
   "source": [
    "The guarantees that MLPs can model any function\n",
    "or that RNNs can model Turing machines\n",
    "provide decent intuition but are not directly practically useful.\n",
    "Among other reasons, they don't guarantee learnability --\n",
    "that starting from random parameters we can find the parameters\n",
    "that implement a given function.\n",
    "The\n",
    "[effective capacity of neural networks is much lower](https://arxiv.org/abs/1901.09021)\n",
    "than would seem from basic theoretical and empirical analysis.\n",
    "\n",
    "One way of understanding capacity to model language is\n",
    "[the Chomsky hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy).\n",
    "In this model of formal languages,\n",
    "Turing machines sit at the top\n",
    "([practically speaking](https://arxiv.org/abs/math/0209332)).\n",
    "\n",
    "With better mathematical models,\n",
    "RNNs and LSTMs can be shown to be\n",
    "[much weaker within the Chomsky hierarchy](https://arxiv.org/abs/2102.10094),\n",
    "with RNNs looking more like\n",
    "[a regex parser](https://en.wikipedia.org/wiki/Finite-state_machine#Acceptors)\n",
    "and LSTMs coming in\n",
    "[just above them](https://en.wikipedia.org/wiki/Counter_automaton).\n",
    "\n",
    "More controversially:\n",
    "the Chomsky hierarchy is great for understanding syntax and grammar,\n",
    "which makes it great for building parsers\n",
    "and working with formal languages,\n",
    "but the goal in _natural_ language processing is to understand _natural_ language.\n",
    "Most humans' natural language is far from strictly grammatical,\n",
    "but that doesn't mean it is nonsense.\n",
    "\n",
    "And to really \"understand\" language means\n",
    "to understand its semantic content, which is fuzzy.\n",
    "The most important thing for handling the fuzzy semantic content\n",
    "of language is not whether you can recall\n",
    "[a parenthesis arbitrarily far in the past](https://en.wikipedia.org/wiki/Dyck_language)\n",
    "but whether you can model probabilistic relationships between concepts\n",
    "in addition to grammar and syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These both leave theoretical room for improvement over current recurrent\n",
    "language and sequence models.\n",
    "\n",
    "But the real cause of the rise of Transformers is that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dsu1ebvAp-3Z"
   },
   "source": [
    "## Transformers are designed to train fast at scale on contemporary hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4abU5adsPGs"
   },
   "source": [
    "The Transformer architecture has several important features,\n",
    "discussed below,\n",
    "but one of the most important reasons why it is successful\n",
    "is because it can be more easily trained at scale.\n",
    "\n",
    "This scalability is the focus of the discussion in the paper\n",
    "that introduced the architecture,\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n",
    "and\n",
    "[comes up whenever there's speculation about scaling up recurrent models](https://twitter.com/jekbradbury/status/1550928156504100864).\n",
    "\n",
    "The recursion in RNNs is inherently sequential:\n",
    "the dependence on the outputs from earlier in the sequence\n",
    "means computations within an example cannot be parallelized.\n",
    "\n",
    "So RNNs must batch across examples to scale,\n",
    "but as sequence length grows this hits memorybandwidth limits.\n",
    "Serving up large batches quickly with good randomness guarantees\n",
    "is also hard to optimize,\n",
    "especially in distributed settings.\n",
    "\n",
    "The Transformer architecture,\n",
    "on the other hand,\n",
    "can be readily parallelized within a single example sequence,\n",
    "in addition to parallelization across batches.\n",
    "This can lead to massive performance gains for a fixed scale,\n",
    "which means larger, higher capacity models\n",
    "can be trained on larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mzk2haFC_G1"
   },
   "source": [
    "How does the architecture achieve this parallelizability?\n",
    "\n",
    "Let's start with the architecture diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u59eu4snLQfp"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "base_url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com\"\n",
    "\n",
    "display.Image(url=base_url + \"/aiayn-figure-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez-XEQ7M0UlR"
   },
   "source": [
    "> <small> To head off a bit of confusion\n",
    "    in case you've worked with Transformer architectures before:\n",
    "    the original \"Transformer\" is an encoder/decoder architecture.\n",
    "    Many LLMs, like GPT models, are decoder only,\n",
    "    because this has turned out to scale well,\n",
    "    and in NLP you can always just make the inputs part of the \"outputs\" by prepending --\n",
    "    it's all text anyways.\n",
    "    We, however, will be using them across modalities,\n",
    "    so we need an explicit encoder,\n",
    "    as above. </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ok4ksBi4vp89"
   },
   "source": [
    "First focusing on the encoder (left):\n",
    "the encoding at a given position is a function of all previous inputs.\n",
    "But it is not a function of the previous _encodings_:\n",
    "we produce the encodings \"all at once\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPN7C-_OqzHP"
   },
   "source": [
    "The decoder (right) does use previous \"outputs\" as its inputs,\n",
    "but those outputs are not the vectors of layer activations\n",
    "(aka embeddings)\n",
    "that are produced by the network.\n",
    "They are instead the processed outputs,\n",
    "after a `softmax` and an `argmax`.\n",
    "\n",
    "We could obtain these outputs by processing the embeddings,\n",
    "much like in a recurrent architecture.\n",
    "In fact, that is one way that Transformers are run.\n",
    "It's what happens in the `.forward` method\n",
    "of the model we'll be training for character recognition:\n",
    "`ResnetTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5_2WMmtDnJn"
   },
   "source": [
    "Let's look at that forward method\n",
    "and connect it to the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FR5pk4kEyCGg"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.models import ResnetTransformer\n",
    "\n",
    "\n",
    "ResnetTransformer.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J5UFDoPzPbq"
   },
   "source": [
    "`.encode` happens first -- that's the left side of diagram.\n",
    "\n",
    "The encoder can in principle be anything\n",
    "that produces a sequence of fixed-length vectors,\n",
    "but here it's\n",
    "[a `ResNet` implementation from `torchvision`](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Then we start iterating over the sequence\n",
    "in the `for` loop.\n",
    "\n",
    "Focus on the first few lines of code.\n",
    "We apply `.decode` (right side of diagram)\n",
    "to the outputs so far.\n",
    "\n",
    "Once we have a new `output`, we apply `.argmax`\n",
    "to turn the logits into a concrete prediction of\n",
    "a particular token.\n",
    "\n",
    "This is added as the last output token\n",
    "and then the loop happens again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTcy8-rV1dHr"
   },
   "source": [
    "Run this way, our model looks very much like a recurrent architecture:\n",
    "we call the model on its own outputs\n",
    "to generate the next value.\n",
    "These types of models are also referred to as\n",
    "[autoregressive models](https://deepgenerativemodels.github.io/notes/autoregressive/),\n",
    "because we predict (as we do in _regression_)\n",
    "the next value based on our own (_auto_) output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Transformers are designed to be _trained_ more scalably than RNNs,\n",
    "not necessarily to _run inference_ more scalably,\n",
    "and it's actually not the case that our model's `.forward` is called during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCxMSAWmEKBt"
   },
   "source": [
    "Let's look at what happens during training\n",
    "by checking the `training_step`\n",
    "of the `LightningModule`\n",
    "we use to train our Transformer models,\n",
    "the `TransformerLitModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o7q8N7P2w4H"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models import TransformerLitModel\n",
    "\n",
    "TransformerLitModel.training_step??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VgNNOjvzC4y"
   },
   "source": [
    "Notice that we call `.teacher_forward` on the inputs, instead of `model.forward`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz-6NGPR4dUr"
   },
   "source": [
    "Let's look at `.teacher_forward`,\n",
    "and in particular its type signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILc2oWET4i2Z"
   },
   "outputs": [],
   "source": [
    "TransformerLitModel.teacher_forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses both inputs `x` _and_ ground truth targets `y` to produce the `outputs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf32lpgrDb__"
   },
   "source": [
    "This is known as \"teacher forcing\".\n",
    "The \"teacher\" signal is \"forcing\"\n",
    "the model to behave as though\n",
    "it got the answer right.\n",
    "\n",
    "[Teacher forcing was originally developed for RNNs](https://direct.mit.edu/neco/article-abstract/1/2/270/5490/A-Learning-Algorithm-for-Continually-Running-Fully).\n",
    "It's more effective here\n",
    "because the right teaching signal\n",
    "for our network is the target data,\n",
    "which we have access to during training,\n",
    "whereas in an RNN the best teaching signal\n",
    "would be the target embedding vector,\n",
    "which we do not know.\n",
    "\n",
    "During inference, when we don't have access to the ground truth,\n",
    "we revert to the autoregressive `.forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"trick\" allows Transformer architectures to readily scale\n",
    "up models to the parameter counts\n",
    "[required to make full use of internet-scale datasets](https://arxiv.org/abs/2001.08361)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAjqpJm9uUuU"
   },
   "source": [
    "## Is there more to Transformers more than just a training trick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWCYXeHv7Qc9"
   },
   "source": [
    "[Very](https://arxiv.org/abs/2005.14165),\n",
    "[very](https://arxiv.org/abs/1909.08053),\n",
    "[very](https://arxiv.org/abs/2205.01068)\n",
    "large Transformer models have powered the most recent wave of exciting results in ML, like\n",
    "[photorealistic high-definition image generation](https://cdn.openai.com/papers/dall-e-2.pdf).\n",
    "\n",
    "They are also the first machine learning models to have come anywhere close to\n",
    "deserving the term _artificial intelligence_ --\n",
    "a slippery concept, but \"how many Turing-type tests do you pass?\" is a good barometer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is surprising because the models and their training procedure are\n",
    "(relatively speaking)\n",
    "pretty _simple_,\n",
    "even if it doesn't feel that way on first pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic Transformer architecture is just a bunch of\n",
    "dense matrix multiplications and non-linearities --\n",
    "it's perhaps simpler than a convolutional architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And advances since the introduction of Transformers in 2017\n",
    "have not in the main been made by\n",
    "creating more sophisticated model architectures\n",
    "but by increasing the scale of the base architecture,\n",
    "or if anything making it simpler, as in\n",
    "[GPT-type models](https://arxiv.org/abs/2005.14165),\n",
    "which drop the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1HQS9ey8GMc"
   },
   "source": [
    "These models are also trained on very simple tasks:\n",
    "most LLMs are just trying to predict the next element in the sequence,\n",
    "given the previous elements --\n",
    "a task simple enough that Claude Shannon,\n",
    "father of information theory, was\n",
    "[able to work on it in the 1950s](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf).\n",
    "\n",
    "These tasks are chosen because it is easy to obtain extremely large-scale datasets,\n",
    "e.g. by scraping the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are also trained in a simple fashion:\n",
    "first-order stochastic optimizers, like SGD or an\n",
    "[ADAM variant](https://optimization.cbe.cornell.edu/index.php?title=Adam),\n",
    "intended for the most basic of optimization problems,\n",
    "that scale more readily than the second-order optimizers\n",
    "that dominate other areas of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz9HPDoy7OAl"
   },
   "source": [
    "This is\n",
    "[the bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)\n",
    "of work in ML:\n",
    "simple, even seemingly wasteful,\n",
    "architectures that scale well and are robust\n",
    "to implementation details\n",
    "eventually outstrip more clever but\n",
    "also more finicky approaches that are harder to scale.\n",
    "This lesson has led some to declare that\n",
    "[scale is all you need](https://fsdl-public-assets.s3.us-west-2.amazonaws.com/siayn.jpg)\n",
    "in machine learning, and perhaps even in artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdN9o2Y771YZ"
   },
   "source": [
    "> <small> That is not to say that because the algorithms are relatively simple,\n",
    "    training a model at this scale is _easy_ --\n",
    "    [datasets require cleaning](https://openreview.net/forum?id=UoEw6KigkUn),\n",
    "    [model architectures require tuning and hyperparameter selection](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2),\n",
    "    [distributed systems require care and feeding](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf).\n",
    "    But choosing the simplest algorithm at every step makes solving the scaling problem feasible.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baVGf6gKFOvs"
   },
   "source": [
    "The importance of scale is the key lesson from the Transformer architecture,\n",
    "far more than any theoretical considerations\n",
    "or any of the implementation details.\n",
    "\n",
    "That said, these large Transformer models are capable of\n",
    "impressive behaviors and understanding how they achieve them\n",
    "is of intellectual interest.\n",
    "Furthermore, like any architecture,\n",
    "there are common failure modes,\n",
    "of the model and of the modelers who use them,\n",
    "that need to be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t2Cfq9Fq67Q"
   },
   "source": [
    "Below, we'll cover two key intuitions about Transformers:\n",
    "Transformers are _residual_, like ResNets,\n",
    "and they compose _low rank_ sequence transformations.\n",
    "Together, this means they act somewhat like a computer,\n",
    "reading from and writing to a \"tape\" or memory\n",
    "with a sequence of simple instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t2Cfq9Fq67Q"
   },
   "source": [
    "We'll also cover a surprising implementation detail:\n",
    "despite being commonly used for sequence modeling,\n",
    "by default the architecture is _position insensitive_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uni0VTCr9lev"
   },
   "source": [
    "### Intuition #1: Transformers are highly residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MoBt-JLJz-d"
   },
   "source": [
    "> The discussion of these inuitions summarizes the discussion in\n",
    "[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)\n",
    "from\n",
    "[Anthropic](https://www.anthropic.com/),\n",
    "an AI safety and research company.\n",
    "The figures below are from that blog post.\n",
    "It is the spiritual successor to the\n",
    "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
    "covered in\n",
    "[Lab 02b](https://lab02b-colab).\n",
    "If you want to truly understand Transformers,\n",
    "we highly recommend you check it out,\n",
    "including the\n",
    "[associated exercises](https://transformer-circuits.pub/2021/exercises/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUbNVvM5Ferm"
   },
   "source": [
    "It's easy to see that ResNets are residual --\n",
    "it's in the name, after all.\n",
    "\n",
    "But Transformers are,\n",
    "in some sense,\n",
    "even more closely tied to residual computation\n",
    "than are ResNets:\n",
    "ResNets and related architectures include downsampling,\n",
    "so there is not a direct path from inputs to outputs.\n",
    "\n",
    "In Transformers, the exact same shape is maintained\n",
    "from the moment tokens are embedded,\n",
    "through dozens or hundreds of intermediate layers,\n",
    "and until they are \"unembedded\" into class logits.\n",
    "The Transformer Circuits authors refer to this pathway as the \"residual stream\".\n",
    "\n",
    "The resiudal stream is easy to see with a change of perspective.\n",
    "Instead of the usual architecture diagram above,\n",
    "which emphasizes the layers acting on the tensors,\n",
    "consider this alternative view,\n",
    "which emphasizes the tensors as they pass through the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRMlVguKKW6y"
   },
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/transformer-residual-view.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9K3N7ilVkB3"
   },
   "source": [
    "For definitions of variables and terms, see the\n",
    "[notation reference here](https://transformer-circuits.pub/2021/framework/index.html#notation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arvciE-kKd_L"
   },
   "source": [
    "Note that this is a _decoder-only_ Transformer architecture --\n",
    "so it should be compared with the right-hand side of the original architecture diagram above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvrRMd_RKp_G"
   },
   "source": [
    "Notice that outputs of the attention blocks \n",
    "and of the MLP layers are\n",
    "added to their inputs, as in a ResNet.\n",
    "These operations are represented as \"Add & Norm\" layers in the classical diagram;\n",
    "normalization is ignored here for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8n_iT-FFAbK"
   },
   "source": [
    "This total commitment to residual operations\n",
    "means the size of the embeddings\n",
    "(referred to as the \"model dimension\" or the \"embedding dimension\",\n",
    "here and below `d_model`)\n",
    "stays the same throughout the entire network.\n",
    "\n",
    "That means, for example,\n",
    "that the output of each layer can be used as input to the \"unembedding\" layer\n",
    "that produces logits.\n",
    "We can read out the computations of intermediate layers\n",
    "just by passing them through the unembedding layer\n",
    "and examining the logit tensor.\n",
    "See\n",
    "[\"interpreting GPT: the logit lens\"](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)\n",
    "for detailed experiments and interactive notebooks.\n",
    "\n",
    "In short, we observe a sort of \"progressive refinement\"\n",
    "of the next-token prediction\n",
    "as the embeddings proceed, depthwise, through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ovh_3YgY9z2h"
   },
   "source": [
    "### Intuition #2 Transformer heads learn low rank transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpNmozlnOdPC"
   },
   "source": [
    "In the original paper and in\n",
    "most presentations of Transformers,\n",
    "the attention layer is written like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA7me8gNP5LE"
   },
   "outputs": [],
   "source": [
    "display.Latex(r\"$\\text{softmax}(Q \\cdot K^T) \\cdot V$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pseudo-typed PyTorch (based loosely on\n",
    "[`torchtyping`](https://github.com/patrick-kidger/torchtyping))\n",
    "that looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oeict_6wGJgD"
   },
   "source": [
    "```python\n",
    "def classic_attention(\n",
    "    Q: torch.Tensor[\"d_sequence\", \"d_model\"],\n",
    "    K: torch.Tensor[\"d_sequence\", \"d_model\"],\n",
    "    V: torch.Tensor[\"d_sequence\", \"d_model\"]) -> torch.Tensor[\"d_sequence\", \"d_model\"]:\n",
    "    return torch.softmax(Q @ K.T) @ V\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pewU90DSuOR"
   },
   "source": [
    "This is effectively exactly\n",
    "how it is written\n",
    "in PyTorch,\n",
    "apart from implementation details\n",
    "(look for `bmm` for the matrix multiplications and a `softmax` call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrgTpKFvOhwc"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F._scaled_dot_product_attention??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebDXZ0tlSe7g"
   },
   "source": [
    "But the best way to write an operation so that a computer can execute it quickly\n",
    "is not necessarily the best way to write it so that a human can understand it --\n",
    "otherwise we'd all be coding in assembly.\n",
    "\n",
    "And this is a strange way to write it --\n",
    "you'll notice that what we normally think of\n",
    "as the \"inputs\" to the layer are not shown.\n",
    "\n",
    "We can instead write out the attention layer\n",
    "as a function of the inputs $x$.\n",
    "We write it for a single \"attention head\".\n",
    "Each attention layer includes a number of heads\n",
    "that read and write from the residual stream\n",
    "simultaneously and independently.\n",
    "We also add the output layer weights $W_O$\n",
    "and we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuFNR67tQpsf"
   },
   "outputs": [],
   "source": [
    "display.Latex(r\"$\\text{softmax}(\\underbrace{x^TW_Q^T}_Q \\underbrace{W_Kx}_{K^T}) \\underbrace{x W_V^T}_V W_O^T$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVnBjjfOLwxP"
   },
   "source": [
    "or, in pseudo-typed PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmpOm-HfGaNz"
   },
   "source": [
    "```python\n",
    "def rewrite_attention_single_head(x: torch.Tensor[\"d_sequence\", \"d_model\"]) -> torch.Tensor[\"d_sequence\", \"d_model\"]:\n",
    "    query_weights: torch.Tensor[\"d_head\", \"d_model\"] = W_Q\n",
    "    key_weights: torch.Tensor[\"d_head\", \"d_model\"] = W_K\n",
    "    key_query_circuit: torch.Tensor[\"d_model\", \"d_model\"] = W_Q.T @ W_K\n",
    "    # maps queries of residual stream to keys from residual stream, independent of position\n",
    "\n",
    "    value_weights: torch.Tensor[\"d_head\", \"d_model\"] = W_V\n",
    "    output_weights: torch.Tensor[\"d_model\", \"d_head\"] = W_O\n",
    "    value_output_circuit: torch.Tensor[\"d_model\", \"d_model\"] = W_V.T @ W_O.T\n",
    "    # transformation applied to each token, regardless of position\n",
    "\n",
    "    attention_logits = x.T @ key_query_circuit @ x\n",
    "    attention_map: torch.Tensor[\"d_sequence\", \"d_sequence\"] = torch.softmax(attention_logits)\n",
    "    # maps positions to positions, often very sparse\n",
    "\n",
    "    value_output: torch.Tensor[\"d_sequence\", \"d_model\"] = x @ value_output_circuit\n",
    "\n",
    "    return attention_map @ value_output  # transformed tokens filtered by attention map\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dC0eqxZ6UAGT"
   },
   "source": [
    "Consider the `key_query_circuit`\n",
    "and `value_output_circuit`\n",
    "matrices, $W_{QK} := W_Q^TW_K$ and $W_{OV}^T := W_V^TW_O^T$\n",
    "\n",
    "The key/query dimension, `d_head`\n",
    "is small relative to the model's dimension, `d_model`,\n",
    "so $W_{QK}$ and $W_{OV}$ are very low rank,\n",
    "[which is the same as saying](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Decomposition_rank)\n",
    "that they factorize into two matrices,\n",
    "one with a smaller number of rows\n",
    "and another with a smaller number of columns.\n",
    "That number is called the _rank_.\n",
    "\n",
    "When computing, these matrices are better represented via their components,\n",
    "rather than computed directly,\n",
    "which leads to the normal implementation of attention.\n",
    "\n",
    "In a large language model,\n",
    "the ratio of residual stream dimension, `d_model`, to\n",
    "the dimension of a single head, `d_head`, is huge, often 100:1.\n",
    "That means each query, key, and value computed at a position\n",
    "is a fairly simple, low-dimensional feature of the residual stream at that position.\n",
    "\n",
    "For visual intuition,\n",
    "we compare what a matrix with a rank 100th of full rank looks like,\n",
    "relative to a full rank matrix of the same size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LUbojJMiW2C"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "low_rank = torch.randn(100, 1) @ torch.randn(1, 100)\n",
    "full_rank = torch.randn(100, 100)\n",
    "plt.figure(); plt.title(\"rank 1/100 matrix\"); plt.imshow(low_rank, cmap=\"Greys\"); plt.axis(\"off\")\n",
    "plt.figure(); plt.title(\"rank 100/100 matrix\");  plt.imshow(full_rank, cmap=\"Greys\"); plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqBst92-OVka"
   },
   "source": [
    "The pattern in the first matrix is very simple,\n",
    "relative to the pattern in the second matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkCGrs9EiVh4"
   },
   "source": [
    "Another feature of low rank transformations is\n",
    "that they have a large nullspace or kernel --\n",
    "these are directions we can move the input without changing the output.\n",
    "\n",
    "That means that many changes to the residual stream won't affect the behavior of this head at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVz2dQgzhD4p"
   },
   "source": [
    "### Residuality and low rank together make Transformers less like a sequence model and more like a computer (that we can take gradients through)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVlzwR03m8mC"
   },
   "source": [
    "The combination of residuality\n",
    "(changes are added to the current input)\n",
    "and low rank\n",
    "(only a small subspace is changed by each head)\n",
    "drastically changes the intuition about Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqjZI2jKe6HH"
   },
   "source": [
    "Rather than being an \"embedding of a token in its context\",\n",
    "the residual stream becomes something more like a memory or a scratchpad:\n",
    "one layer reads a small bit of information from the stream\n",
    "and writes a small bit of information back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YIBkxlqepjc"
   },
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/transformer-layer-residual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtsKhkLfk00l"
   },
   "source": [
    "The residual stream works like a memory because it is roomy enough\n",
    "that these actions need not interfere:\n",
    "the subspaces targeted by reads and writes are small relative to the ambient space,\n",
    "so they can\n",
    "\n",
    "Additionally, the dimension of each head is still in the 100s in large models,\n",
    "and\n",
    "[high dimensional (>50) vector spaces have many \"almost-orthogonal\" vectors](https://link.springer.com/article/10.1007/s12559-009-9009-8)\n",
    "in them, so the number of effectively degrees of freedom is\n",
    "actually larger than the dimension.\n",
    "This phenomenon allows high-dimensional tensors to serve as\n",
    "[very large content-addressable associative memories](https://arxiv.org/abs/2008.06996).\n",
    "There are\n",
    "[close connections between associative memory addressing algorithms and Transformer attention](https://arxiv.org/abs/2008.02217).\n",
    "\n",
    "Together, this means an early layer can write information to the stream\n",
    "that can be used by later layers -- by many of them at once, possibly much later.\n",
    "Later layers can learn to edit this information,\n",
    "e.g. deleting it,\n",
    "if doing so reduces the loss,\n",
    "but by default the information is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EragIygzJg86"
   },
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/residual-stream-read-write.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKIaUZjwkpW7"
   },
   "source": [
    "Lastly, the softmax in the attention has a sparsifying effect,\n",
    "and so many attention heads are reading from \n",
    "just one token and writing to just one other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dN6VcJqIMKnB"
   },
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/residual-token-to-token.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeatedly reading information from an external memory\n",
    "and using it to decide which operation to perform\n",
    "and where to write the results\n",
    "is at the core of the\n",
    "[Turing machine formalism](https://en.wikipedia.org/wiki/Turing_machine).\n",
    "For a concrete example, the\n",
    "[Transformer Circuits work](https://transformer-circuits.pub/2021/framework/index.html)\n",
    "includes a dissection of a form of \"pointer arithmetic\"\n",
    "that appears in some models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kLFh7Mvnolr"
   },
   "source": [
    "This point of view seems\n",
    "very promising for explaining numerous\n",
    "otherwise perhaps counterintuitive features of Transformer models.\n",
    "\n",
    "- This framework predicts lots that Transformers will readily copy-and-paste information,\n",
    "which might explain phenomena like\n",
    "[incompletely trained Transformers repeating their outputs multiple times](https://youtu.be/SQLm9U0L0zM?t=1030).\n",
    "\n",
    "- It also readily explains\n",
    "[in-context learning behavior](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html),\n",
    "an important component of why Transformers perform well on medium-length texts\n",
    "and in few-shot learning.\n",
    "\n",
    "- Transformers also perform better on reasoning tasks when the text\n",
    "[\"let's think step-by-step\"](https://arxiv.org/abs/2205.11916)\n",
    "is added to their input prompt.\n",
    "This is partly due to the fact that that prompt is associated,\n",
    "in the dataset, with clearer reasoning,\n",
    "and since the models are trained to predict which tokens tend to appear\n",
    "after an input, they tend to produce better reasoning with that prompt --\n",
    "an explanation purely in terms of sequence modeling.\n",
    "But it also gives the Transformer license to generate a large number of tokens\n",
    "that act to store intermediate information,\n",
    "making for a richer residual stream\n",
    "for reading and writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyLRzgG-93yB"
   },
   "source": [
    "### Implementation detail: Transformers are position-insensitive by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR6PnrlA_hJ2"
   },
   "source": [
    "In the attention calculation\n",
    "each token can query each other token,\n",
    "with no regard for order.\n",
    "Furthermore, the construction of queries, keys, and values\n",
    "is based on the content of the embedding vector,\n",
    "which does not automatically include its position.\n",
    "\"dog bites man\" and \"man bites dog\" are identical, as in\n",
    "[bag-of-words modeling](https://machinelearningmastery.com/gentle-introduction-bag-words-model/).\n",
    "\n",
    "For most sequences,\n",
    "this is unacceptable:\n",
    "absolute and relative position matter\n",
    "and we cannot use the future to predict the past.\n",
    "\n",
    "We need to add two pieces to get a Transformer architecture that's usable for next-token prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWHxGJz2-6ZK"
   },
   "source": [
    "First, the simpler piece:\n",
    "\"causal\" attention,\n",
    "so-named because it ensures that values earlier in the sequence\n",
    "are not influenced by later values, which would\n",
    "[violate causality](https://youtu.be/4xj0KRqzo-0?t=42)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c42xi6URYB4"
   },
   "source": [
    "The most common solution is straightforward:\n",
    "we calculate attention between all tokens,\n",
    "then throw out non-causal values by \"masking\" them\n",
    "(this is before applying the softmax,\n",
    "so masking means adding $-\\infty$).\n",
    "\n",
    "This feels wasteful --\n",
    "why are we calculating values we don't need?\n",
    "Trying to be smarter would be harder,\n",
    "and might rely on operations that aren't as optimized as\n",
    "matrix multiplication and addition.\n",
    "Furthermore, it's \"only\" twice as many operations,\n",
    "so it doesn't even show up in $O$-notation.\n",
    "\n",
    "A sample attention mask generated by our code base is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXaWe6pT-9jV"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.models import transformer_util\n",
    "\n",
    "\n",
    "attention_mask = transformer_util.generate_square_subsequent_mask(100)\n",
    "\n",
    "ax = plt.matshow(torch.exp(attention_mask.T));  cb = plt.colorbar(ticks=[0, 1], fraction=0.05)\n",
    "plt.ylabel(\"Can the embedding at this index\"); plt.xlabel(\"attend to embeddings at this index?\")\n",
    "print(attention_mask[:10, :10].T); cb.set_ticklabels([False, True]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solves our causality problem,\n",
    "but we still don't have positional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZamUE4WIoGS2"
   },
   "source": [
    "The standard technique\n",
    "is to add alternating sines and cosines\n",
    "of increasing frequency to the embeddings\n",
    "(there are\n",
    "[others](https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00445/111478/Position-Information-in-Transformers-An-Overview),\n",
    "most notably\n",
    "[rotary embeddings](https://blog.eleuther.ai/rotary-embeddings/)).\n",
    "Each position in the sequence is then uniquely identifiable\n",
    "from the pattern of these values.\n",
    "\n",
    "> <small> Furthermore, for the same reason that\n",
    "    [translation-equivariant convolutions are related to Fourier transforms](https://math.stackexchange.com/questions/918345/fourier-transform-as-diagonalization-of-convolution),\n",
    "    translations, e.g. relative positions, are fairly easy to express as linear transformations\n",
    "    of sines and cosines).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDG2uOsaELU0"
   },
   "source": [
    "We superimpose this positional information on our embeddings.\n",
    "Note that because the model is residual,\n",
    "this position information will be by default preserved\n",
    "as it passes through the network,\n",
    "so it doesn't need to be repeatedly added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what this positional encoding looks like in our codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Zk62Q-a-1Ax"
   },
   "outputs": [],
   "source": [
    "PositionalEncoder = transformer_util.PositionalEncoding(d_model=50, dropout=0.0, max_len=200)\n",
    "\n",
    "pe = PositionalEncoder.pe.squeeze().T[:, :]  # placing sequence dimension along the \"x-axis\"\n",
    "\n",
    "ax = plt.matshow(pe); plt.colorbar(ticks=[-1, 0, 1], fraction=0.05)\n",
    "plt.xlabel(\"sequence index\"); plt.ylabel(\"embedding dimension\"); plt.title(\"Positional Encoding\", y=1.1)\n",
    "print(pe[:4, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep2ClIWvqDms"
   },
   "source": [
    "When we add the positional information to our embeddings,\n",
    "both the embedding information and the positional information\n",
    "is approximately preserved,\n",
    "as can be visually assessed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJuFjoCzC0Y4"
   },
   "outputs": [],
   "source": [
    "fake_embeddings = torch.randn_like(pe) * 0.5\n",
    "\n",
    "ax = plt.matshow(fake_embeddings); plt.colorbar(ticks=torch.arange(-2, 3), fraction=0.05)\n",
    "plt.xlabel(\"sequence index\"); plt.ylabel(\"embedding dimension\"); plt.title(\"Embeddings Without Positional Encoding\", y=1.1)\n",
    "\n",
    "fake_embeddings_with_pe = fake_embeddings + pe\n",
    "\n",
    "plt.matshow(fake_embeddings_with_pe); plt.colorbar(ticks=torch.arange(-2, 3), fraction=0.05)\n",
    "plt.xlabel(\"sequence index\"); plt.ylabel(\"embedding dimension\"); plt.title(\"Embeddings With Positional Encoding\", y=1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHIzBxDkEmH8"
   },
   "source": [
    "A [similar technique](https://arxiv.org/abs/2103.06450)\n",
    "is used to also incorporate positional information into the image embeddings,\n",
    "which are flattened before being fed to the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC1N85wl8dvn"
   },
   "source": [
    "### Learn more about Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJwYxkjTk15t"
   },
   "source": [
    "We're only able to give a flavor and an intuition for Transformers here.\n",
    "\n",
    "To improve your grasp on the nuts and bolts, check out the\n",
    "[original \"Attention Is All You Need\" paper](https://arxiv.org/abs/1706.03762),\n",
    "which is surprisingly approachable,\n",
    "as far as ML research papers go.\n",
    "The\n",
    "[Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "adds code and commentary to the original paper,\n",
    "which makes it even more digestible.\n",
    "For something even friendlier, check out the\n",
    "[Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "by Jay Alammar, which has an accompanying\n",
    "[video](https://youtu.be/-QH8fRhqFHM).\n",
    "\n",
    "Anthropic's work on\n",
    "[Transformer Circuits](https://transformer-circuits.pub/),\n",
    "summarized above, has some of the best material\n",
    "for building theoretical understanding\n",
    "and is still being updated with extensions and applications of the framework.\n",
    "The\n",
    "[accompanying exercises](https://transformer-circuits.pub/2021/exercises/index.html)\n",
    "are a great aid for checking and building your understanding.\n",
    "\n",
    "But they are fairly math-heavy.\n",
    "If you have more of a software engineering background, see\n",
    "Transformer Circuits co-author Nelson Elhage's blog post\n",
    "[Transformers for Software Engineers](https://blog.nelhage.com/post/transformers-for-software-engineers/).\n",
    "\n",
    "For a gentler introduction to the intuition for Transformers,\n",
    "check out Brandon Rohrer's\n",
    "[Transformers From Scratch](https://e2eml.school/transformers.html)\n",
    "tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg7zntJES-aT"
   },
   "source": [
    "An aside:\n",
    "the matrix multiplications inside attention dominate\n",
    "the big-$O$ runtime of Transformers.\n",
    "So trying to make the attention mechanism more efficient, e.g. linear time,\n",
    "has generated a lot of research\n",
    "(review paper\n",
    "[here](https://arxiv.org/abs/2009.06732)).\n",
    "Despite drawing a lot of attention, so to speak,\n",
    "at the time of writing in mid-2022, these methods\n",
    "[haven't been used in large language models](https://twitter.com/MitchellAGordon/status/1545932726775193601),\n",
    "so it isn't likely to be worth the effort to spend time learning about them\n",
    "unless you are a Transformer specialist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCjXysEJ8g9_"
   },
   "source": [
    "# Using Transformers to read paragraphs of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsfKWnOvqjva"
   },
   "source": [
    "Our simple convolutional model for text recognition from\n",
    "[Lab 02b](https://fsdl.me/lab02b-colab)\n",
    "could only handle cleanly-separated characters.\n",
    "\n",
    "It worked by sliding a LeNet-style CNN\n",
    "over the image,\n",
    "predicting a character for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njLdzBqy-I90"
   },
   "outputs": [],
   "source": [
    "import text_recognizer.data\n",
    "\n",
    "\n",
    "emnist_lines = text_recognizer.data.EMNISTLines()\n",
    "line_cnn = text_recognizer.models.LineCNNSimple(emnist_lines.config())\n",
    "\n",
    "# for sliding, see the for loop over range(S)\n",
    "line_cnn.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0N6yDBQq8ns"
   },
   "source": [
    "But unfortunately for us, handwritten text\n",
    "doesn't come in neatly-separated characters\n",
    "of equal size, so we trained our model on synthetic data\n",
    "designed to work with that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiqUVbj0sxLr"
   },
   "source": [
    "Now that we have a better model,\n",
    "we can work with better data:\n",
    "paragraphs from the\n",
    "[IAM Handwriting database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oizsOAcKs-dD"
   },
   "source": [
    "The cell uses our `LightningDataModule`\n",
    "to download and preprocess this data,\n",
    "writing results to disk.\n",
    "We can then spin up `DataLoader`s to give us batches.\n",
    "\n",
    "It can take several minutes to run the first time\n",
    "on commodity machines,\n",
    "with most time spent extracting the data.\n",
    "On subsequent runs,\n",
    "the time-consuming operations will not be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uL9LHbjdsUbm"
   },
   "outputs": [],
   "source": [
    "iam_paragraphs = text_recognizer.data.IAMParagraphs()\n",
    "\n",
    "iam_paragraphs.prepare_data()\n",
    "iam_paragraphs.setup()\n",
    "xs, ys = next(iter(iam_paragraphs.val_dataloader()))\n",
    "\n",
    "iam_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBkFN9bbTm_S"
   },
   "source": [
    "Now that we've got a batch,\n",
    "let's take a look at some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqaps8yxtBhU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "\n",
    "def show(y):\n",
    "    y = y.detach().cpu()  # bring back from accelerator if it's being used\n",
    "    return \"\".join(np.array(iam_paragraphs.mapping)[y]).replace(\"<P>\", \"\")\n",
    "\n",
    "idx = random.randint(0, len(xs))\n",
    "\n",
    "print(show(ys[idx]))\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dT3UCNzTsoc"
   },
   "source": [
    "The `ResnetTransformer` model can run on this data\n",
    "if passed the `.config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXL-vIGRr86D"
   },
   "outputs": [],
   "source": [
    "import text_recognizer.models\n",
    "\n",
    "\n",
    "rnt = text_recognizer.models.ResnetTransformer(data_config=iam_paragraphs.config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMxa-oWyT01E"
   },
   "source": [
    "Our models are now big enough\n",
    "that we want to make use of GPU acceleration\n",
    "as much as we can,\n",
    "even when working on single inputs,\n",
    "so let's cast to the GPU if we have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YyUM8LgvW0w"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "rnt.to(device); xs = xs.to(device); ys = ys.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-E3UdD4zUJi"
   },
   "source": [
    "First, let's just pass it through the ResNet encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LUUtlvaxrvg"
   },
   "outputs": [],
   "source": [
    "resnet_embedding, = rnt.resnet(xs[idx:idx+1].repeat(1, 3, 1, 1))\n",
    " # resnet is designed for RGB images, so we replicate the input across channels 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eimgJ5dnywjg"
   },
   "outputs": [],
   "source": [
    "resnet_idx = random.randint(0, len(resnet_embedding))  # re-execute to view a different channel\n",
    "plt.matshow(resnet_embedding[resnet_idx].detach().cpu(), cmap=\"Greys_r\");\n",
    "plt.axis(\"off\"); plt.colorbar(fraction=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings, though generated by random, untrained weights,\n",
    "are not entirely useless.\n",
    "\n",
    "Before neural networks could be effectively\n",
    "trained end to end,\n",
    "they were often used with frozen random weights\n",
    "eveywhere except the final layer\n",
    "(see e.g.\n",
    "[Echo State Networks](http://www.scholarpedia.org/article/Echo_state_network)).\n",
    "[As late as 2015](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W13/html/Paisitkriangkrai_Effective_Semantic_Pixel_2015_CVPR_paper.html),\n",
    "these methods were still competitive, and\n",
    "[Neural Tangent Kernels](https://arxiv.org/abs/1806.07572)\n",
    "provide a\n",
    "[theoretical basis](https://arxiv.org/abs/2011.14522)\n",
    "for understanding their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye6pW0ETzw2A"
   },
   "source": [
    "The final result, though, is repetitive gibberish --\n",
    "at the bare minimum, we need to train the unembedding/readout layer\n",
    "in order to get reasonable text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture includes randomization with dropout,\n",
    "so repeated runs of the cell below will generate different outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu3Pa7gLsFMo"
   },
   "outputs": [],
   "source": [
    "preds, = rnt(xs[idx:idx+1])  # can take up to two minutes on a CPU. Transformers ❤️ GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvCXUbskv6XM"
   },
   "outputs": [],
   "source": [
    "print(show(preds.cpu()))\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without teacher forcing, runtime is also variable from iteration to iteration --\n",
    "the model stops when it generates an \"end sequence\" or padding token,\n",
    "which is not deterministic thanks to the dropout layers.\n",
    "For similar reasons, runtime is variable across inputs.\n",
    "\n",
    "The variable runtime of autoregressive generation\n",
    "is also not great for scaling.\n",
    "In a distributed setting, as required for large scale,\n",
    "forward passes need to be synced across devices,\n",
    "and if one device is generating a batch of much longer sequences,\n",
    "it will cause all the others to idle while they wait on it to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t76MSVRXV0V7"
   },
   "source": [
    "Let's turn our model into a `TransformerLitModel`\n",
    "so we can run with teacher forcing.\n",
    "\n",
    "> <small> You may be wondering:\n",
    "    why isn't teacher forcing part of the PyTorch module?\n",
    "    In general, the `LightningModule`\n",
    "    should encapsulate things that are needed in training, validation, and testing\n",
    "    but not during inference.\n",
    "    The teacher forcing trick fits this paradigm,\n",
    "    even though it's so critical to what makes Transformers powerful. </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qrHRKHowdDi"
   },
   "outputs": [],
   "source": [
    "import text_recognizer.lit_models\n",
    "\n",
    "lit_rnt = text_recognizer.lit_models.TransformerLitModel(rnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNaFqR50Oid"
   },
   "source": [
    "Now we can use `.teacher_forward` if we also provide the target `ys`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpZdqXS5wn0F"
   },
   "outputs": [],
   "source": [
    "forcing_outs, = lit_rnt.teacher_forward(xs[idx:idx+1], ys[idx:idx+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zx9SmsN0QLT"
   },
   "source": [
    "This may not run faster than the `rnt.forward`,\n",
    "since generations are always the maximum possible length,\n",
    "but runtimes and output lengths are deterministic and constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu-XNYpi0Qvi"
   },
   "source": [
    "Forcing doesn't necessarily make our predictions better.\n",
    "They remain highly repetitive gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcEgify9w0sv"
   },
   "outputs": [],
   "source": [
    "forcing_preds = torch.argmax(forcing_outs, dim=0)\n",
    "\n",
    "print(show(forcing_preds.cpu()))\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn6GGNzc9a3o"
   },
   "source": [
    "## Training the `ResNetTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvZYsuSyWUXe"
   },
   "source": [
    "We're finally ready to train this model on full paragraphs of handwritten text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cJwC7b720Sd"
   },
   "source": [
    "This is a more serious model --\n",
    "it's the one we use in the\n",
    "[deployed TextRecognizer application](http://fsdl.me/app).\n",
    "It's much larger than the models we've seen this far,\n",
    "so it can easily outstrip available compute resources,\n",
    "in particular GPU memory.\n",
    "\n",
    "To help, we use\n",
    "[automatic mixed precision](https://pytorch-lightning.readthedocs.io/en/1.6.3/advanced/precision.html),\n",
    "which shrinks the size of most of our floats by half,\n",
    "which reduces memory consumption and can speed up computation.\n",
    "\n",
    "If your GPU has less than 8GB of available RAM,\n",
    "you'll see a \"CUDA out of memory\" `RuntimeError`,\n",
    "which is something of a\n",
    "[rite of passage in ML](https://twitter.com/Suhail/status/1549555136350982145).\n",
    "In this case, you can resolve it by reducing the `--batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1mXlhfy04Nm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gpus = int(torch.cuda.is_available())\n",
    "\n",
    "if gpus:\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"watch out! working with this model on a typical CPU is not feasible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os1vW1rPZ1dy"
   },
   "source": [
    "Even with an okay GPU, like a\n",
    "[Tesla P100](https://www.nvidia.com/en-us/data-center/tesla-p100/),\n",
    "a single epoch of training can take over 10 minutes to run.\n",
    "We use the `--limit_{train/val/test}_batches` flags to keep the runtime short,\n",
    "but you can remove those flags to see what full training looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnF6dWFn4JlZ"
   },
   "source": [
    "It can take a long time (overnight)\n",
    "to train this model to decent performance on a single GPU,\n",
    "so we'll focus on other pieces for the exercises.\n",
    "\n",
    "> <small> At the time of writing in mid-2022, the cheapest readily available option\n",
    "for training this model to decent performance on this dataset with this codebase\n",
    "comes out around $10, using\n",
    "[the 8xV100 instance on Lambda Labs' GPU Cloud](https://lambdalabs.com/service/gpu-cloud).\n",
    "See, for example,\n",
    "[this dashboard](https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Training-Run-2022-06-02--VmlldzoyMTAyOTkw)\n",
    "and associated experiment.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HufjdUZN0t4l",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# above %%magic times the cell, useful as a poor man's profiler\n",
    "\n",
    "%run training/run_experiment.py --data_class IAMParagraphs --model_class ResnetTransformer --loss transformer \\\n",
    "  --gpus={gpus} --batch_size 16 --precision 16 \\\n",
    "  --limit_train_batches 10 --limit_test_batches 1 --limit_val_batches 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6fQ93ju3Iku"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udb1Ekjx3L63"
   },
   "source": [
    "### 🌟 Try out gradient accumulation and other \"training tricks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpqViB4p3Wfb"
   },
   "source": [
    "Larger batches are helpful not only for increasing parallelization\n",
    "and amortizing fixed costs\n",
    "but also for getting more reliable gradients.\n",
    "Larger batches give gradients with less noise\n",
    "and to a point, less gradient noise means faster convergence.\n",
    "\n",
    "But larger batches result in larger tensors,\n",
    "which take up more GPU memory,\n",
    "a resource that is tightly constrained\n",
    "and device-dependent.\n",
    "\n",
    "Does that mean we are limited in the quality of our gradients\n",
    "due to our machine size?\n",
    "\n",
    "Not entirely:\n",
    "look up the `--accumulate_grad_batches`\n",
    "argument to the `pl.Trainer`.\n",
    "You should be able to understand why\n",
    "it makes it possible to compute the same gradients\n",
    "you would find for a batch of size `k * N`\n",
    "on a machine that can only run batches up to size `N`.\n",
    "\n",
    "Accumulating gradients across batches is among the\n",
    "[advanced training tricks supported by Lightning](https://pytorch-lightning.readthedocs.io/en/1.6.3/advanced/training_tricks.html).\n",
    "Try some of them out!\n",
    "Keep the `--limit_{blah}_batches` flags in place so you can quickly experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2vtkmX830y3"
   },
   "source": [
    "### 🌟🌟 Find the smallest model that can still fit a single batch of 16 examples.\n",
    "\n",
    "While training this model to actually fit the whole dataset is infeasible\n",
    "as a short exercise on commodity hardware,\n",
    "it's practical to train this model to memorize a batch of 16 examples.\n",
    "\n",
    "Passing `--overfit_batches 1` flag limits the number of training batches to 1\n",
    "and turns off\n",
    "[`DataLoader` shuffling](https://discuss.pytorch.org/t/how-does-shuffle-in-data-loader-work/49756)\n",
    "so that in each epoch, the model just sees the same single batch of data over and over again.\n",
    "\n",
    "At first, try training the model to a loss of `2.5` --\n",
    "it should be doable in 100 epochs or less,\n",
    "which is just a few minutes on a commodity GPU.\n",
    "\n",
    "Once you've got that working,\n",
    "crank up the number of epochs by a factor of 10\n",
    "and confirm that the loss continues to go down.\n",
    "\n",
    "Some tips:\n",
    "\n",
    "- Use `--limit_test_batches 0` to turn off testing.\n",
    "We don't need it because we don't care about generalization\n",
    "and it's relatively slow because it runs the model autoregressively.\n",
    "\n",
    "- Use `--help` and look through the model class args\n",
    "to find the arguments used to reduce model size.\n",
    "\n",
    "- By default, there's lots of regularization to prevent overfitting.\n",
    "Look through the args for the model class and data class\n",
    "for regularization knobs to turn off or down."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab03_transformers.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
