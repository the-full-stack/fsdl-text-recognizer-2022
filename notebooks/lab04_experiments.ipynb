{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 04: Experiment Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- How experiment management brings observability to ML model development\n",
    "- Which features of experiment management we use in developing the Text Recognizer\n",
    "- Workflows for using Weights & Biases in experiment management, including metric logging, artifact versioning, and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 4\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab contains a large number of embedded iframes\n",
    "that benefit from having a wide window.\n",
    "The cell below makes the notebook as wide as your browser window\n",
    "if `full_width` is set to `True`.\n",
    "Full width is the default behavior in Colab,\n",
    "so this cell is intended to improve the viewing experience in other Jupyter environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow along with a video walkthrough on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"https://fsdl.me/2022-lab-04-video-embed\", width=\"50%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoFCoEcC8SV"
   },
   "source": [
    "# Why experiment management?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why we need experiment management for ML development,\n",
    "let's start by running an experiment.\n",
    "\n",
    "We'll train a new model on a new dataset,\n",
    "using the training script `training/run_experiment.py`\n",
    "introduced in [Lab 02a](https://fsdl.me/lab02a-colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a CNN encoder and Transformer decoder, as in\n",
    "[Lab 03](https://fsdl.me/lab03-colab),\n",
    "but with some changes so we can iterate faster.\n",
    "We'll operate on just single lines of text at a time (`--dataclass IAMLines`), as in\n",
    "[Lab02b](https://fsdl.me/lab02b-colab),\n",
    "and we'll use a smaller CNN (`--modelclass LineCNNTransformer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.data.iam import IAM  # base dataset of images of handwritten text\n",
    "from text_recognizer.data import IAMLines  # processed version split into individual lines\n",
    "from text_recognizer.models import LineCNNTransformer  # simple CNN encoder / Transformer decoder\n",
    "\n",
    "\n",
    "print(IAM.__doc__)\n",
    "\n",
    "# uncomment a line below for details on either class\n",
    "# IAMLines??  \n",
    "# LineCNNTransformer??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will train a model on 10% of the data for two epochs.\n",
    "\n",
    "It takes up to a few minutes to run on commodity hardware,\n",
    "including data download and preprocessing.\n",
    "As it's running, continue reading below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "\n",
    "gpus = int(torch.cuda.is_available()) \n",
    "\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 2 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 --limit_test_batches 0.1 --log_every_n_steps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model trains, we're calculating lots of metrics --\n",
    "loss on training and validation, [character error rate](https://torchmetrics.readthedocs.io/en/v0.7.3/references/functional.html#char-error-rate-func) --\n",
    "and reporting them to the terminal.\n",
    "\n",
    "This is achieved by the built-in `.log` method\n",
    "([docs](https://pytorch-lightning.readthedocs.io/en/1.6.1/common/lightning_module.html#train-epoch-level-metrics))\n",
    "of the `LightningModule`,\n",
    "and it is a very straightforward way to get basic information about your experiment as it's running\n",
    "without leaving the context where you're running it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to read\n",
    "[information from streaming numbers in the command line](http://www.quickmeme.com/img/45/4502c7603faf94c0e431761368e9573df164fad15f1bbc27fc03ad493f010dea.jpg)\n",
    "is something of a rite of passage for MLEs, but\n",
    "let's consider what we can't see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're missing all metric values except the most recent --\n",
    "we can see them as they stream in, but they're constantly overwritten.\n",
    "We also can't associate them with timestamps, steps, or epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We also don't see any system metrics.\n",
    "We can't see how much the GPU is being utilized, how much CPU RAM is free, or how saturated our I/O bandwidth is\n",
    "without launching a separate process.\n",
    "And even if we do, those values will also not be saved and timestamped,\n",
    "so we can't correlate them with other things during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we continue to run experiments, changing code and opening new terminals,\n",
    "even the information we have or could figure out now will disappear.\n",
    "Say you spot a weird error message during training,\n",
    "but your session ends and the stdout is gone,\n",
    "so you don't know exactly what it was.\n",
    "Can you recreate the error?\n",
    "Which git branch and commit were you on?\n",
    "Did you have any uncommitted changes? Which arguments did you pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also, model checkpoints containing the parameter values have been saved to disk.\n",
    "Can we relate these checkpoints to their metrics, both in terms of accuracy and in terms of performance?\n",
    "As we run more and more experiments,\n",
    "we'll want to slice and dice them to see if,\n",
    "say, models with `--lr 0.001` are generally better or worse than models with `--lr 0.0001`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save and log all of this information, and more, in order to make our model training\n",
    "[observable](https://docs.honeycomb.io/getting-started/learning-about-observability/) --\n",
    "in short, so that we can understand, make decisions about, and debug our model training\n",
    "by looking at logs and source code, without having to recreate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had to write the logging code we need to save this information ourselves, that'd put us in for a world of hurt:\n",
    "1. That's a lot of code that's not at the core of building an ML-powered system. Robustly saving version control information means becoming _very_ good with your VCS, which is less time spent on mastering the important stuff -- your data, your models, and your problem domain.\n",
    "2. It's very easy to forget to log something that you don't yet realize is going to be critical at some point. Data on network traffic, disk I/O, and GPU/CPU syncing is unimportant until suddenly your training has slowed to a crawl 12 hours into training and you can't figure out where the bottleneck is.\n",
    "3. Once you do start logging everything that's necessary, you might find it's not performant enough -- the code you wrote so you can debug performance issues is [tanking your performance](https://i.imgflip.com/6q54og.jpg).\n",
    "4. Just logging is not enough. The bytes of data need to be made legible to humans in a GUI and searchable via an API, or else they'll be too hard to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Experiment Tracking with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we don't have to. PyTorch Lightning integrates with other libraries for additional logging features,\n",
    "and it makes logging very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.log` method of the `LightningModule` isn't just for logging to the terminal.\n",
    "\n",
    "It can also use a logger to push information elsewhere.\n",
    "\n",
    "By default, we use\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard)\n",
    "via the Lightning `TensorBoardLogger`,\n",
    "which has been saving results to the local disk.\n",
    "\n",
    "Let's find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a sequence of bash commands to get the latest experiment's directory\n",
    "#  by hand, you can just copy and paste it from the terminal\n",
    "\n",
    "list_all_log_files = \"find training/logs/lightning_logs/\"  # find avoids issues ls has with \\n in filenames\n",
    "filter_to_folders = \"grep '_[0-9]*$'\"  # regex match on end of line\n",
    "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
    "take_first = \"head -n 1\"  # the first n elements, n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_log, = ! {list_all_log_files} | {filter_to_folders} | {sort_version_descending} | {take_first}\n",
    "latest_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -lh {latest_log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view results, we need to launch a TensorBoard server --\n",
    "much like we need to launch a Jupyter server to use Jupyter notebooks.\n",
    "\n",
    "The cells below load an extension that lets you use TensorBoard inside of a notebook\n",
    "the same way you'd use it from the command line, and then launch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# same command works in terminal, with \"{arguments}\" replaced with values or \"$VARIABLES\"\n",
    "\n",
    "port = 11717  # pick an open port on your machine\n",
    "host = \"0.0.0.0\" # allow connections from the internet\n",
    "                 #   watch out! make sure you turn TensorBoard off\n",
    "\n",
    "%tensorboard --logdir {latest_log} --port {port} --host {host}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some charts of metrics over time along with some charting controls.\n",
    "\n",
    "You can click around in this interface and explore it if you'd like,\n",
    "but in the next section, we'll see that there are better tools for experiment management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've run many experiments on this machine,\n",
    "you can see all of their results by pointing TensorBoard\n",
    "at the whole `lightning_logs` directory,\n",
    "rather than just one experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir training/logs/lightning_logs --port {port + 1} --host \"0.0.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large numbers of experiments, the management experience is not great --\n",
    "it's for example hard to go from a line in a chart to metadata about the experiment or metric depicted in that line.\n",
    "\n",
    "It's especially difficult to switch between types of experiments, to compare experiments run on different machines, or to collaborate with others,\n",
    "which are important workflows as applications mature and teams grow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard is an independent service, so we need to make sure we turn it off when we're done. Just flip `done_with_tensorboard` to `True`.\n",
    "\n",
    "If you run into any issues with the above cells failing to launch,\n",
    "especially across iterations of this lab, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard.manager\n",
    "\n",
    "# get the process IDs for all tensorboard instances\n",
    "pids = [tb.pid for tb in tensorboard.manager.get_all()]\n",
    "\n",
    "done_with_tensorboard = False\n",
    "\n",
    "if done_with_tensorboard:\n",
    "    # kill processes\n",
    "    for pid in pids:\n",
    "        !kill {pid} 2> /dev/null\n",
    "        \n",
    "    # remove the temporary files that sometimes persist, see https://stackoverflow.com/a/59582163\n",
    "    !rm -rf {tensorboard.manager._get_info_dir()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Management with Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we manage experiments when we hit the limits of local TensorBoard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is powerful and flexible and very scalable,\n",
    "but running it requires engineering effort and babysitting --\n",
    "you're running a database, writing data to it,\n",
    "and layering a web application over it.\n",
    "\n",
    "This is a fairly common workflow for web developers,\n",
    "but not so much for ML engineers.\n",
    "\n",
    "You can avoid this with [tensorboard.dev](https://tensorboard.dev/),\n",
    "and it's as simple as running the command `tensorboard dev upload`\n",
    "pointed at your logging directory.\n",
    "\n",
    "But there are strict limits to this free service:\n",
    "1GB of tensor data and 1GB of binary data.\n",
    "A single Text Recognizer model checkpoint is ~100MB,\n",
    "and that's not particularly large for a useful model.\n",
    "\n",
    "Furthermore, all data is public,\n",
    "so if you upload the inputs and outputs of your model,\n",
    "anyone who finds the link can see them.\n",
    "\n",
    "Overall, tensorboard.dev works very well for certain academic and open projects\n",
    "but not for industrial ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid that narrow permissions and limits issue,\n",
    "you could use [git LFS](https://git-lfs.github.com/)\n",
    "to track the binary data and tensor data,\n",
    "which is more likely to be sensitive than metrics.\n",
    "\n",
    "The Hugging Face ecosystem uses TensorBoard and git LFS.\n",
    "\n",
    "It includes the Hugging Face Hub, a git server much like GitHub,\n",
    "but designed first and foremost for collaboration on models and datasets,\n",
    "rather than collaboration on code.\n",
    "For example, the Hugging Face Hub\n",
    "[will host TensorBoard alongside models](https://huggingface.co/docs/hub/tensorboard)\n",
    "and officially has\n",
    "[no storage limit](https://discuss.huggingface.co/t/is-there-a-size-limit-for-dataset-hosting/14861/4),\n",
    "avoiding the\n",
    "[bandwidth and storage pricing](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage)\n",
    "that make using git LFS with GitHub expensive.\n",
    "\n",
    "However, we prefer to avoid mixing software version control and experiment management.\n",
    "\n",
    "First, using the Hub requires maintaining an additional git remote,\n",
    "which is a hard ask for many engineering teams.\n",
    "\n",
    "Secondly, git-style versioning is an awkward fit for logging --\n",
    "is it really sensible to create a new commit for each logging event while you're watching live?\n",
    "\n",
    "Instead, we prefer to use systems that solve experiment management with _databases_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple alternatives to TensorBoard + git LFS that fit this bill.\n",
    "The primary [open governance](https://www.ibm.com/blogs/cloud-computing/2016/10/27/open-source-open-governance/)\n",
    "tool is [MLflow](https://github.com/mlflow/mlflow/)\n",
    "and there are a number of\n",
    "[closed-governance and/or closed-source tools](https://www.reddit.com/r/MachineLearning/comments/q5g7m9/n_sagemaker_experiments_vs_comet_neptune_wandb_etc/).\n",
    "\n",
    "These tools generally avoid any need to worry about hosting\n",
    "(unless data governance rules require a self-hosted version).\n",
    "\n",
    "For a sampling of publicly-posted opinions on experiment management tools,\n",
    "see these discussions from Reddit:\n",
    "\n",
    "- r/mlops: [1](https://www.reddit.com/r/mlops/comments/uxieq3/is_weights_and_biases_worth_the_money/), [2](https://www.reddit.com/r/mlops/comments/sbtkxz/best_mlops_platform_for_2022/)\n",
    "- r/MachineLearning: [3](https://www.reddit.com/r/MachineLearning/comments/sqa36p/comment/hwls9px/?utm_source=share&utm_medium=web2x&context=3)\n",
    "\n",
    "Among these tools, the FSDL recommendation is\n",
    "[Weights & Biases](https://wandb.ai),\n",
    "which we believe offers\n",
    "- the best user experience, both in the Python SDKs and in the graphical interface\n",
    "- the best integrations with other tools,\n",
    "including\n",
    "[Lightning](https://docs.wandb.ai/guides/integrations/lightning) and\n",
    "[Keras](https://docs.wandb.ai/guides/integrations/keras),\n",
    "[Jupyter](https://docs.wandb.ai/guides/track/jupyter),\n",
    "and even\n",
    "[TensorBoard](https://docs.wandb.ai/guides/integrations/tensorboard),\n",
    "and\n",
    "- the best tools for collaboration.\n",
    "\n",
    "Below, we'll take care to point out which logging and management features\n",
    "are available via generic interfaces in Lightning and which are W&B-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "print(wandb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding it to our experiment running code is extremely easy,\n",
    "relative to the features we get, which is\n",
    "one of the main selling points of W&B.\n",
    "\n",
    "We get most of our new experiment management features just by changing a single variable, `logger`, from\n",
    "`TensorboardLogger` to `WandbLogger`\n",
    "and adding two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"args.wandb\" -A 5 training/run_experiment.py | head -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see what each of these lines does for us below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this logger is built into and maintained by PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "WandbLogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the rest of this notebook,\n",
    "you'll need a Weights & Biases account.\n",
    "\n",
    "As with GitHub the free tier, for personal, academic, and open source work,\n",
    "is very generous.\n",
    "\n",
    "The Text Recognizer project will fit comfortably within the free tier.\n",
    "\n",
    "Run the cell below and follow the prompts to log in or create an account or go\n",
    "[here](https://wandb.ai/signup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to launch an experiment tracked with Weights & Biases.\n",
    "\n",
    "The experiment can take between 3 and 10 minutes to run.\n",
    "In that time, continue reading below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 10 \\\n",
    "  --log_every_n_steps 10 --wandb --limit_test_batches 0.1 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1\n",
    "    \n",
    "last_expt = wandb.run\n",
    "\n",
    "wandb.finish()  # necessary in this style of in-notebook experiment running, not necessary in CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some new things in our output.\n",
    "\n",
    "For example, there's a note from `wandb` that the data is saved locally\n",
    "and also synced to their servers.\n",
    "\n",
    "There's a link to a webpage for viewing the logged data and a name for our experiment --\n",
    "something like `dandy-sunset-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local logging and cloud syncing happens with minimal impact on performance,\n",
    "because `wandb` launches a separate process to listen for events and upload them.\n",
    "\n",
    "That's a table-stakes feature for a logging framework but not a pleasant thing to write in Python yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view results, head to the link in the notebook output\n",
    "that looks like \"Syncing run **{adjective}-{noun}-{number}**\".\n",
    "\n",
    "There's no need to wait for training to finish.\n",
    "\n",
    "The next sections describe the contents of that interface. You can read them while looking at the W&B interface in a separate tab or window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even more convenience, once training is finished we can also see the results directly in the notebook by embedding the webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_expt.url)\n",
    "IFrame(last_expt.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have landed on the run page\n",
    "([docs](https://docs.wandb.ai/ref/app/pages/run-page)),\n",
    "which collects up all of the information for a single experiment into a collection of tabs.\n",
    "\n",
    "We'll work through these tabs from top to bottom.\n",
    "\n",
    "Each header is also a link to the documentation for a tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Overview tab](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab)\n",
    "This tab has an icon that looks like `(i)` or 🛈.\n",
    "\n",
    "The top section of this tab has high-level information about our run:\n",
    "- Timing information, like start time and duration\n",
    "- System hardware, hostname, and basic environment info\n",
    "- Git repository link and state\n",
    "\n",
    "This information is collected and logged automatically.\n",
    "\n",
    "The section at the bottom contains configuration information, which here includes all CLI args or their defaults,\n",
    "and summary metrics.\n",
    "\n",
    "Configuration information is collected with `.log_hyperparams` in Lightning or `wandb.config` otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Charts tab](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab)\n",
    "\n",
    "This tab has a line plot icon, something like 📈.\n",
    "\n",
    "It's also the default page you land on when looking at a W&B run.\n",
    "\n",
    "Charts are generated for everything we `.log` from PyTorch Lightning. The charts here are interactive and editable, and changes persist.\n",
    "\n",
    "Unfurl the \"Gradients\" section in this tab to check out the gradient histograms. These histograms can be useful for debugging training instability issues.\n",
    "\n",
    "We were able to log these just by calling `wandb.watch` on our model. This is a W&B-specific feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [System tab](https://docs.wandb.ai/ref/app/pages/run-page#system-tab)\n",
    "This tab has computer chip icon.\n",
    "\n",
    "It contains\n",
    "- GPU metrics for all GPUs: temperature, [utilization](https://stackoverflow.com/questions/5086814/how-is-gpu-and-memory-utilization-defined-in-nvidia-smi-results), and memory allocation\n",
    "- CPU metrics: memory usage, utilization, thread counts\n",
    "- Disk and network I/O levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model tab](https://docs.wandb.ai/ref/app/pages/run-page#model-tab)\n",
    "This tab has an undirected graph icon that looks suspiciously like a [pawnbrokers' symbol](https://en.wikipedia.org/wiki/Pawnbroker#:~:text=The%20pawnbrokers%27%20symbol%20is%20three,the%20name%20of%20Lombard%20banking.).\n",
    "\n",
    "The information here was also generated from `wandb.watch`, and includes parameter counts and input/output shapes for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Logs tab](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab)\n",
    "This tab has an icon that looks like a stylized command prompt, `>_`.\n",
    "\n",
    "It contains information that was printed to the stdout.\n",
    "\n",
    "This tab is useful for, e.g., determining when exactly a warning or error message started appearing.\n",
    "\n",
    "Note that model summary information is printed here. We achieve this with a Lightning `Callback` called `ModelSummary`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"callbacks.ModelSummary\" training/run_experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning `Callback`s add extra \"nice-to-have\" engineering features to our model training.\n",
    "\n",
    "For more on Lightning `Callback`s, see\n",
    "[Lab 02a](https://fsdl.me/lab02a-colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Files tab](https://docs.wandb.ai/ref/app/pages/run-page#files-tab)\n",
    "This tab has a stylized document icon, something like 📄.\n",
    "\n",
    "You can use this tab to view any files saved with the `wandb.save`.\n",
    "\n",
    "For most uses, that style is deprecated in favor of `wandb.log_artifact`,\n",
    "which we'll discuss shortly.\n",
    "\n",
    "But a few pieces of information automatically collected by W&B end up in this tab.\n",
    "\n",
    "Some highlights:\n",
    "  - Much more detailed environment info: `conda-environment.yaml` and `requirements.txt`\n",
    "  - A `diff.patch` that represents the difference between the files in the `git` commit logged in the overview and the actual disk state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Artifacts tab](https://docs.wandb.ai/ref/app/pages/run-page#artifacts-tab)\n",
    "This tab has the database or [drum memory icon](https://stackoverflow.com/a/2822750), which looks like a cylinder of three stacked hockey pucks.\n",
    "\n",
    "This tab contains all of the versioned binary files, aka artifacts, associated with our run.\n",
    "\n",
    "We store two kinds of binary files\n",
    "  - `run_table`s of model inputs and outputs\n",
    "  - `model` checkpoints\n",
    "\n",
    "We get model checkpoints via the built-in Lightning `ModelCheckpoint` callback, which is not specific to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"callbacks.ModelCheckpoint\" -A 9 training/run_experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools for working with artifacts in W&B are powerful and complex, so we'll cover them in various places throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Tables of Logged Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the Charts tab,\n",
    "notice that we have model inputs and outputs logged in structured tables\n",
    "under the train, validation, and test sections.\n",
    "\n",
    "These tables are interactive as well\n",
    "([docs](https://docs.wandb.ai/guides/data-vis/log-tables)).\n",
    "They support basic exploratory data analysis and are compatible with W&B's collaboration features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to charts in our run page, these tables also have their own pages inside the W&B web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_versions_url = last_expt.url.split(\"runs\")[0] + f\"artifacts/run_table/run-{last_expt.id}-trainpredictions/\"\n",
    "table_data_url = table_versions_url + \"v0/files/train/predictions.table.json\"\n",
    "\n",
    "print(table_data_url)\n",
    "IFrame(src=table_data_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting this to work requires more effort and more W&B-specific code\n",
    "than the other features we've seen so far.\n",
    "\n",
    "We'll briefly explain the implementation here, for those who are interested.\n",
    "\n",
    "We use a custom Lightning `Callback`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.callbacks.imtotext import ImageToTextTableLogger\n",
    "\n",
    "\n",
    "ImageToTextTableLogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Lightning returns logged information on every batch and these outputs are accumulated throughout an epoch.\n",
    "\n",
    "The values are then aggregated with a frequency determined by the `pl.Trainer` argument `--log_every_n_batches`.\n",
    "\n",
    "This behavior is sensible for metrics, which are low overhead, but not so much for media,\n",
    "where we'd rather subsample and avoid holding on to too much information.\n",
    "\n",
    "So we additionally control when media is included in the outputs with methods like `add_on_logged_batches`.\n",
    "\n",
    "The frequency of media logging is then controlled with `--log_every_n_batches`, as with aggregate metric reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models.base import BaseImageToTextLitModel\n",
    "\n",
    "BaseImageToTextLitModel.add_on_logged_batches??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we've seen so far has been related to a single run or experiment.\n",
    "\n",
    "Experiment management starts to shine when you can organize, filter, and group many experiments at once.\n",
    "\n",
    "We organize our runs into \"projects\" and view them on the  W&B \"project page\" \n",
    "([docs](https://docs.wandb.ai/ref/app/pages/project-page)).\n",
    "\n",
    "By default in the Lightning integration, the project name is determined based on directory information.\n",
    "This default can be over-ridden in the code when creating a `WandbLogger`,\n",
    "but we find it easier to change it from the command line by setting the `WANDB_PROJECT` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the project page looks like for a longer-running project with lots of experiments.\n",
    "\n",
    "The cell below pulls up the project page for some of the debugging and feature addition work done while updating the course from 2021 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/workspace\"\n",
    "\n",
    "print(project_url)\n",
    "IFrame(src=project_url, width=\"100%\", height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page and these charts have been customized -- filtering down to the most interesting training runs and surfacing the most important high-level information about them.\n",
    "\n",
    "We welcome you to poke around in this interface: deactivate or change the filters, clicking through into individual runs, and change the charts around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond logging metrics and metadata from runs,\n",
    "we can also log and version large binary files, or artifacts, and their metadata ([docs](https://docs.wandb.ai/guides/artifacts/artifacts-core-concepts))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below pulls up all of the artifacts associated with the experiment we just ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IFrame(src=last_expt.url + \"/artifacts\", width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on one of the `model` checkpoints -- the specific version doesn't matter.\n",
    "\n",
    "There are a number of tabs here.\n",
    "\n",
    "The \"Overview\" tab includes automatically generated metadata, like which run by which user created this model checkpoint, when, and how much disk space it takes up.\n",
    "\n",
    "The \"Metadata\" tab includes configurable metadata, here hyperparameters and metrics like `validation/cer`,\n",
    "which are added by default by the `WandbLogger`.\n",
    "\n",
    "The \"Files\" tab contains the actual file contents of the artifact.\n",
    "\n",
    "On the left-hand side of the page, you'll see the other versions of the model checkpoint,\n",
    "including some versions that are \"tagged\" with version aliases, like `latest` or `best`.\n",
    "\n",
    "You can click on these to explore the different versions and even directly compare them.\n",
    "\n",
    "If you're particularly interested in this tool, try comparing two versions of the `validation-predictions` artifact, starting from the Files tab and clicking inside it to `validation/predictions.table.json`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artifact storage is part of the W&B free tier.\n",
    "\n",
    "The storage limits, as of August 2022, cover 100GB of Artifacts and experiment data.\n",
    "\n",
    "The former is sufficient to store ~700 model checkpoints for the Text Recognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your data storage and compare it to your limits at this URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_tracker_url = f\"https://wandb.ai/usage/{last_expt.entity}\"\n",
    "\n",
    "print(storage_tracker_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also programmatically access our data and metadata via the `wandb` API\n",
    "([docs](https://docs.wandb.ai/guides/track/public-api-guide)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can access the metrics we just logged as a `pandas.DataFrame` by grabbing the run via the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wb_api.run(\"/\".join( # fetch a run given\n",
    "    [last_expt.entity,     # the user or org it was logged to\n",
    "     last_expt.project,    # the \"project\", usually one of several per repo/application\n",
    "     last_expt.id]         # and a unique ID\n",
    "))\n",
    "\n",
    "hist = run.history()  # and pull down a sample of the data as a pandas DataFrame\n",
    "\n",
    "hist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.groupby(\"epoch\")[\"train/loss\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this includes the artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which artifacts where created and logged?\n",
    "artifacts = run.logged_artifacts()\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(f\"artifact of type {artifact.type}: {artifact.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our `ImageToTextTableLogger`,\n",
    "we can easily recreate training or validation data that came out of our `DataLoader`s,\n",
    "which is normally ephemeral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "artifact = wb_api.artifact(f\"{last_expt.entity}/{last_expt.project}/run-{last_expt.id}-trainpredictions:latest\")\n",
    "artifact_dir = Path(artifact.download(root=\"training/logs\"))\n",
    "image_dir = artifact_dir / \"media\" / \"images\"\n",
    "\n",
    "images = [path for path in image_dir.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(str(random.choice(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced W&B API Usage: MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of a well-instrumented experiment tracking system is that it allows\n",
    "automatic relation of information:\n",
    "what were the inputs when this model's gradient spiked?\n",
    "Which models have been trained on this dataset,\n",
    "and what was their performance?\n",
    "\n",
    "Having access and automation around this information is necessary for \"MLOps\",\n",
    "which applies contemporary DevOps principles to ML projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below pull down the training data\n",
    "for the model currently running the FSDL Text Recognizer app.\n",
    "\n",
    "This is just intended as a demonstration of what's possible,\n",
    "so don't worry about understanding every piece of this,\n",
    "and feel free to skip past it.\n",
    "\n",
    "MLOps is still a nascent field, and these tools and workflows are likely to change.\n",
    "\n",
    "For example, just before the course launched, W&B released a\n",
    "[Model Registry layer](https://docs.wandb.ai/guides/models)\n",
    "on top of artifact logging that aims to improve the developer experience for these workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the same project we looked at in the project view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_recognizer_project = wb_api.project(\"fsdl-text-recognizer-2021-training\", entity=\"cfrye59\")\n",
    "\n",
    "text_recognizer_project  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we search it for the text recognizer model currently being used in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all versions of the text-recognizer ever put into production by...\n",
    "\n",
    "for art_type in text_recognizer_project.artifacts_types(): # looking through all artifact types\n",
    "    if art_type.name == \"prod-ready\":  # for the prod-ready type\n",
    "        # and grabbing the text-recognizer\n",
    "        production_text_recognizers = art_type.collection(\"paragraph-text-recognizer\").versions()\n",
    "\n",
    "# and then get the one that's currently being tested in CI by...\n",
    "for text_recognizer in production_text_recognizers:\n",
    "    if \"ci-test\" in text_recognizer.aliases:  # looking for the one that's labeled as CI-tested\n",
    "        in_prod_text_recognizer = text_recognizer\n",
    "\n",
    "# view its metadata at the url or in the notebook\n",
    "in_prod_text_recognizer_url = text_recognizer_project.url[:-9] + f\"artifacts/{in_prod_text_recognizer.type}/{in_prod_text_recognizer.name.replace(':', '/')}\"\n",
    "\n",
    "print(in_prod_text_recognizer_url)\n",
    "IFrame(src=in_prod_text_recognizer_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From its metadata, we can get information about how it was \"staged\" to be put into production,\n",
    "and in particular which model checkpoint was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_run = in_prod_text_recognizer.logged_by()\n",
    "\n",
    "training_ckpt, = [at for at in staging_run.used_artifacts() if at.type == \"model\"]\n",
    "training_ckpt.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That checkpoint was logged by a training experiment, which is available as metadata.\n",
    "\n",
    "We can look at the training run for that model, either here in the notebook or at its URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_run = training_ckpt.logged_by()\n",
    "print(training_run.url)\n",
    "IFrame(src=training_run.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, we can access logs and metadata about training,\n",
    "confident that we are working with the model that is actually in production.\n",
    "\n",
    "For example, we can pull down the data we logged and analyze it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = training_run.history(samples=10000)\n",
    "training_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = training_results.groupby(\"epoch\")[\"train/loss\"].mean().plot();\n",
    "training_results[\"validation/loss\"].dropna().plot(logy=True); ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "training_results[\"validation/loss\"].dropna().iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The charts and webpages in Weights & Biases\n",
    "are substantially more useful than ephemeral stdouts or raw logs on disk.\n",
    "\n",
    "If you're spun up on the project,\n",
    "they accelerate debugging, exploration, and discovery.\n",
    "\n",
    "If not, they're not so much useful as they are overwhelming.\n",
    "\n",
    "We need to synthesize the raw logged data into information.\n",
    "This helps us communicate our work with other stakeholders,\n",
    "preserve knowledge and prevent repetition of work,\n",
    "and surface insights faster.\n",
    "\n",
    "These workflows are supported by the W&B Reports feature\n",
    "([docs here](https://docs.wandb.ai/guides/reports)),\n",
    "which mix W&B charts and tables with explanatory markdown text and embeds.\n",
    "\n",
    "Below are some common report patterns and\n",
    "use cases and examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the examples are from the FSDL Text Recognizer project.\n",
    "You can find more of them\n",
    "[here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/-Report-of-Reports---VmlldzoyMjEwNDM5),\n",
    "where we've organized them into a report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dashboards are a structured subset of the output from one or more experiments,\n",
    "designed for quickly surfacing issues or insights,\n",
    "like an accuracy or performance regression\n",
    "or a change in the data distribution.\n",
    "\n",
    "Use cases:\n",
    "- show the basic state of ongoing experiment\n",
    "- compare one experiment to another\n",
    "- select the most important charts so you can spin back up into context on a project more quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Training-Run-2022-06-02--VmlldzoyMTAyOTkw\"\n",
    "\n",
    "IFrame(src=dashboard_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Request Documentation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most software codebases,\n",
    "pull requests are a key focal point\n",
    "for units of work that combine\n",
    "short-term communication and long-term information tracking.\n",
    "\n",
    "In ML codebases, it's more difficult to bring\n",
    "sufficient information together to make PRs as useful.\n",
    "At FSDL, we like to add documentary\n",
    "reports with one or a small number of charts\n",
    "that connect logged information in the experiment management system\n",
    "to state in the version control software.\n",
    "\n",
    "Use cases:\n",
    "- communication of results within a team, e.g. code review\n",
    "- record-keeping that links pull request pages to raw logged info and makes it discoverable\n",
    "- improving confidence in PR correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_doc_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Overfit-Check-After-Refactor--VmlldzoyMDY5MjI1\"\n",
    "\n",
    "IFrame(src=bugfix_doc_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog Post Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sufficient effort, the logged data in the experiment management system\n",
    "can be made clear enough to be consumed,\n",
    "sufficiently contextualized to be useful outside the team, and\n",
    "even beautiful.\n",
    "\n",
    "The result is a report that's closer to a blog post than a dashboard or internal document.\n",
    "\n",
    "Use cases:\n",
    "- communication between teams or vertically in large organizations\n",
    "- external technical communication for branding and recruiting\n",
    "- attracting users or contributors\n",
    "\n",
    "Check out this example, from the Craiyon.ai / DALL·E Mini project, by FSDL alumnus\n",
    "[Boris Dayma](https://twitter.com/borisdayma)\n",
    "and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle_mini_blog_url = \"https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA#training-dall-e-mini\"\n",
    "\n",
    "IFrame(src=dalle_mini_blog_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our choices, like the depth of our network, the nonlinearities of our layers,\n",
    "and the learning rate and other parameters of our optimizer, cannot be\n",
    "([easily](https://arxiv.org/abs/1606.04474))\n",
    "chosen by descent of the gradient of a loss function.\n",
    "\n",
    "But these parameters that impact the values of the parameters\n",
    "we directly optimize with gradients, or _hyperparameters_,\n",
    "can still be optimized,\n",
    "essentially by trying options and selecting the values that worked best.\n",
    "\n",
    "In general, you can attain much of the benefit of hyperparameter optimization with minimal effort.\n",
    "\n",
    "Expending more compute can squeeze small amounts of additional validation or test performance\n",
    "that makes for impressive results on leaderboards but typically doesn't translate\n",
    "into better user experience.\n",
    "\n",
    "In general, the FSDL recommendation is to use the hyperparameter optimization workflows\n",
    "built into your other tooling.\n",
    "\n",
    "Weights & Biases makes the most straightforward forms of hyperparameter optimization trivially easy\n",
    "([docs](https://docs.wandb.ai/guides/sweeps)).\n",
    "\n",
    "It also supports a number of more advanced tools, like\n",
    "[Hyperband](https://docs.wandb.ai/guides/sweeps/configuration#early_terminate)\n",
    "for early termination of poorly-performing runs.\n",
    "\n",
    "We can use the same training script and we don't need to run an optimization server.\n",
    "\n",
    "We just need to write a configuration yaml file\n",
    "([docs](https://docs.wandb.ai/guides/sweeps/configuration)),\n",
    "like the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/simple-overfit-sweep.yaml\n",
    "# first we specify what we're sweeping\n",
    "# we specify a program to run\n",
    "program: training/run_experiment.py\n",
    "# we optionally specify how to run it, including setting default arguments\n",
    "command:  \n",
    "    - ${env}\n",
    "    - ${interpreter}\n",
    "    - ${program}\n",
    "    - \"--wandb\"\n",
    "    - \"--overfit_batches\"\n",
    "    - \"1\"\n",
    "    - \"--log_every_n_steps\"\n",
    "    - \"25\"\n",
    "    - \"--max_epochs\"\n",
    "    - \"100\"\n",
    "    - \"--limit_test_batches\"\n",
    "    - \"0\"\n",
    "    - ${args}  # these arguments come from the sweep parameters below\n",
    "\n",
    "# and we specify which parameters to sweep over, what we're optimizing, and how we want to optimize it\n",
    "method: random  # generally, random searches perform well, can also be \"grid\" or \"bayes\"\n",
    "metric:\n",
    "    name: train/loss\n",
    "    goal: minimize\n",
    "parameters:  \n",
    "    # LineCNN hyperparameters\n",
    "    window_width:\n",
    "        values: [8, 16, 32, 64]\n",
    "    window_stride:\n",
    "        values: [4, 8, 16, 32]\n",
    "    # Transformer hyperparameters\n",
    "    tf_layers:\n",
    "        values: [1, 2, 4, 8]\n",
    "    # we can also fix some values, just like we set default arguments\n",
    "    gpus:\n",
    "        value: 1\n",
    "    model_class:\n",
    "        value: LineCNNTransformer\n",
    "    data_class:\n",
    "        value: IAMLines\n",
    "    loss:\n",
    "        value: transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the config we launch a \"controller\":\n",
    "a lightweight process that just decides what hyperparameters to try next\n",
    "and coordinates the heavierweight training.\n",
    "\n",
    "This lives on the W&B servers, so there are no headaches about opening ports for communication,\n",
    "cleaning up when it's done, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sweep training/simple-overfit-sweep.yaml --project fsdl-line-recognizer-2022\n",
    "simple_sweep_id = wb_api.project(\"fsdl-line-recognizer-2022\").sweeps()[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we can launch an \"agent\" to follow the orders of the controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# interrupt twice to terminate this cell if it's running too long,\n",
    "#   it can be over 15 minutes with some hyperparameters\n",
    "\n",
    "!wandb agent --project fsdl-line-recognizer-2022 --entity {wb_api.default_entity} --count=1 {simple_sweep_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell runs only a single experiment, because we provided the `--count` argument with a value of `1`.\n",
    "\n",
    "If not provided, the agent will run forever for random or Bayesian sweeps\n",
    "or until the sweep is terminated, which can be done from the W&B interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agents make for a slick workflow for distributing sweeps across GPUs.\n",
    "\n",
    "We can just change the `CUDA_VISIBLE_DEVICES` environment variable,\n",
    "which controls which GPUs are accessible by a process, to launch\n",
    "parallel agents on separate GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 wandb agent $SWEEP_ID\n",
    "# open another terminal\n",
    "CUDA_VISIBLE_DEVICES=1 wandb agent $SWEEP_ID\n",
    "# and so on\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFx-OhF837Bp"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include optional exercises with the labs for learners who want to dive deeper on specific topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟Contribute to a hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've kicked off a big hyperparameter search on the `LineCNNTransformer` that anyone can join!\n",
    "\n",
    "There are ~10,000,000 potential hyperparameter combinations,\n",
    "and each takes 30 minutes to test,\n",
    "so checking each possibility will take over 500 years of compute time.\n",
    "Best get cracking then!\n",
    "\n",
    "Run the cell below to pull up a dashboard and print the URL where you can check on the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_entity = \"fullstackdeeplearning\"\n",
    "sweep_project = \"fsdl-line-recognizer-2022\"\n",
    "sweep_id = \"e0eo43eu\"\n",
    "sweep_url = f\"https://wandb.ai/{sweep_entity}/{sweep_project}/sweeps/{sweep_id}\"\n",
    "\n",
    "print(sweep_url)\n",
    "IFrame(src=sweep_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve information about the sweep from the API,\n",
    "including the hyperparameters being swept over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_info = wb_api.sweep(\"/\".join([sweep_entity, sweep_project, sweep_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = sweep_info.config[\"parameters\"]\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to contribute to this sweep,\n",
    "run the cell below after changing the count to a number greater than 0.\n",
    "\n",
    "Each iteration runs for 30 minutes if it does not crash,\n",
    "e.g. due to out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0  # off by default, increase it to join in!\n",
    "\n",
    "if count:\n",
    "    !wandb agent {sweep_id} --entity {sweep_entity} --project {sweep_project} --count {count}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Write some manual logging in `wandb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FSDL Text Recognizer codebase,\n",
    "we almost exclusively log to W&B through Lightning,\n",
    "rather than through the `wandb` Python SDK.\n",
    "\n",
    "If you're interested in learning how to use W&B directly, e.g. with another training framework,\n",
    "try out this quick exercise that introduces the key players in the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below starts a run with `wandb.init` and provides configuration hyperparameters with `wandb.config`.\n",
    "\n",
    "It also calculates a `loss` value and saves a text file, `logs/hello.txt`.\n",
    "\n",
    "Add W&B metric and artifact logging to this cell:\n",
    "- use [`wandb.log`](https://docs.wandb.ai/guides/track/log) to log the loss on each step\n",
    "- use [`wandb.log_artifact`](https://docs.wandb.ai/guides/artifacts) to save `logs/hello.txt` in an artifact with the name `hello` and whatever type you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "project = \"trying-wandb\"\n",
    "config = {\"steps\": 50}\n",
    "\n",
    "\n",
    "with wandb.init(project=project, config=config) as run:\n",
    "    steps = wandb.config[\"steps\"]\n",
    "    \n",
    "    for ii in range(steps):\n",
    "        loss = math.exp(-ii) + random.random() / (ii + 1)  # ML means making the loss go down\n",
    "        \n",
    "    with open(\"logs/hello.txt\", \"w\") as f:\n",
    "        f.write(\"hello from wandb, my dudes!\")\n",
    "        \n",
    "    run_id = run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've correctly completed the exercise, the cell below will print only 🥞 emojis and no 🥲s before opening the run in an iframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_run = wb_api.run(f\"{project}/{run_id}\")\n",
    "\n",
    "# check for logged loss data\n",
    "if \"loss\" not in hello_run.history().keys():\n",
    "    print(\"loss not logged 🥲\")\n",
    "else:\n",
    "    print(\"loss logged successfully 🥞\")\n",
    "    if len(hello_run.history()[\"loss\"]) != steps:\n",
    "        print(\"loss not logged on all steps 🥲\")\n",
    "    else:\n",
    "        print(\"loss logged on all steps 🥞\")\n",
    "\n",
    "artifacts =  hello_run.logged_artifacts()\n",
    "\n",
    "# check for artifact with the right name\n",
    "if \"hello:v0\" not in [artifact.name for artifact in artifacts]:\n",
    "    print(\"hello artifact not logged 🥲\")\n",
    "else:\n",
    "    print(\"hello artifact logged successfully 🥞\")\n",
    "    # check for the file inside the artifacts\n",
    "    if \"hello.txt\" not in sum([list(artifact.manifest.entries.keys()) for artifact in artifacts], []):\n",
    "        print(\"could not find hello.txt 🥲\")\n",
    "    else:\n",
    "        print(\"hello.txt logged successfully 🥞\")\n",
    "    \n",
    "    \n",
    "hello_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Find good hyperparameters for the `LineCNNTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default hyperparameters for the `LineCNNTransformer` are not particularly carefully tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and find some better hyperparameters: choices that achieve a lower loss on the full dataset faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe interesting phenomena during training,\n",
    "from promising hyperparameter combos to software bugs to strange model behavior,\n",
    "turn the charts into a W&B report and share it with the FSDL community or\n",
    "[open an issue on GitHub](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022/issues)\n",
    "with a link to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the sweep_info.config above to see the model and data hyperparameters\n",
    "#   read through the --help output for all potential arguments\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 5 \\\n",
    "  --log_every_n_steps 50 --wandb --limit_test_batches 0.1 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 \\\n",
    "  --help  # remove this line to run an experiment instead of printing help\n",
    "    \n",
    "last_hyperparam_expt = wandb.run  # in case you want to pull URLs, look up in API, etc., as in code above\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟🌟🌟 Add logging of tensor statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to logging model inputs and outputs as human-interpretable media,\n",
    "it's also frequently useful to see information about their numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in learning more about metric calculation and logging with Lightning,\n",
    "use [`torchmetrics`](https://torchmetrics.readthedocs.io/en/v0.7.3/)\n",
    "to add tensor statistic logging to the `LineCNNTransformer`.\n",
    "\n",
    "`torchmetrics` comes with built in statistical metrics, like `MinMetric`, `MaxMetric`, and `MeanMetric`.\n",
    "\n",
    "All three are useful, but start by adding just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your metric with `training/run_experiment.py`, you'll need to open and edit the `text_recognizer/lit_model/base.py` and `text_recognizer/lit_model/transformer.py` files\n",
    "- Add the metrics to the `BaseImageToTextLitModel`'s `__init__` method, around where `CharacterErrorRate` appears.\n",
    "  - You'll also need to decide whether to calculate separate train/validation/test versions. Whatever you do, start by implementing just one.\n",
    "- In the appropriate `_step` methods of the `TransformerLitModel`, add metric calculation and logging for `Min`, `Max`, and/or `Mean`.\n",
    "  - Base your code on the calculation and logging of the `val_cer` metric.\n",
    "  - `sync_dist=True` is only important in distributed training settings, so you might not notice any issues regardless of that argument's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an extra challenge, use `MeanSquaredError` to implement a `VarianceMetric`. _Hint_: one way is to use `torch.zeros_like` and `torch.mean`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMKpeodqRUzgu0VjkCVMBeJ",
   "collapsed_sections": [],
   "name": "lab04_experiments.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
